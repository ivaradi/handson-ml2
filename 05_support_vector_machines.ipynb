{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Chapter 5 – Support Vector Machines**\n\n_This notebook contains all the sample code and solutions to the exercises in chapter 5._","metadata":{}},{"cell_type":"markdown","source":"<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml2/blob/master/05_support_vector_machines.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20.","metadata":{}},{"cell_type":"code","source":"# Python ≥3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"svm\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:22:15.425939Z","iopub.execute_input":"2022-03-18T18:22:15.426558Z","iopub.status.idle":"2022-03-18T18:22:16.676065Z","shell.execute_reply.started":"2022-03-18T18:22:15.426429Z","shell.execute_reply":"2022-03-18T18:22:16.675027Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Linear SVM Classification","metadata":{}},{"cell_type":"markdown","source":"The next few code cells generate the first figures in chapter 5. The first actual code sample comes after.\n\n**Code to generate Figure 5–1. Large margin classification**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\n\n# SVM Classifier model\nsvm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\nsvm_clf.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bad models\nx0 = np.linspace(0, 5.5, 2)\npred_1 = 5*x0 - 20\npred_2 = x0 - 1.8\npred_3 = 0.1 * x0 + 0.5\n\ndef plot_svc_decision_boundary(svm_clf, xmin, xmax):\n    w = svm_clf.coef_[0]\n    b = svm_clf.intercept_[0]\n\n    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n    # => x1 = -w0/w1 * x0 - b/w1\n    x0 = np.linspace(xmin, xmax, 2)\n    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n\n    margin = 1/w[1]\n    gutter_up = decision_boundary + margin\n    gutter_down = decision_boundary - margin\n\n    svs = svm_clf.support_vectors_\n    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n    plt.plot(x0, gutter_down, \"k--\", linewidth=2)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(x0, pred_1, \"g--\", linewidth=2)\nplt.plot(x0, pred_2, \"m-\", linewidth=2)\nplt.plot(x0, pred_3, \"r-\", linewidth=2)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\n\nplt.sca(axes[1])\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\n\nsave_fig(\"large_margin_classification_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate Figure 5–2. Sensitivity to feature scales**","metadata":{}},{"cell_type":"code","source":"Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\nys = np.array([0, 0, 1, 1])\nsvm_clf = SVC(kernel=\"linear\", C=100)\nsvm_clf.fit(Xs, ys)\n\nplt.figure(figsize=(9,2.7))\nplt.subplot(121)\nplt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\nplt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\nplot_svc_decision_boundary(svm_clf, 0, 6)\nplt.xlabel(\"$x_0$\", fontsize=20)\nplt.ylabel(\"$x_1$    \", fontsize=20, rotation=0)\nplt.title(\"Unscaled\", fontsize=16)\nplt.axis([0, 6, 0, 90])\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(Xs)\nsvm_clf.fit(X_scaled, ys)\n\nplt.subplot(122)\nplt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\nplt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\nplot_svc_decision_boundary(svm_clf, -2, 2)\nplt.xlabel(\"$x'_0$\", fontsize=20)\nplt.ylabel(\"$x'_1$  \", fontsize=20, rotation=0)\nplt.title(\"Scaled\", fontsize=16)\nplt.axis([-2, 2, -2, 2])\n\nsave_fig(\"sensitivity_to_feature_scales_plot\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Soft Margin Classification\n**Code to generate Figure 5–3. Hard margin sensitivity to outliers**","metadata":{}},{"cell_type":"code","source":"X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\ny_outliers = np.array([0, 0])\nXo1 = np.concatenate([X, X_outliers[:1]], axis=0)\nyo1 = np.concatenate([y, y_outliers[:1]], axis=0)\nXo2 = np.concatenate([X, X_outliers[1:]], axis=0)\nyo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n\nsvm_clf2 = SVC(kernel=\"linear\", C=10**9)\nsvm_clf2.fit(Xo2, yo2)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")\nplt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")\nplt.text(0.3, 1.0, \"Impossible!\", fontsize=24, color=\"red\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[0][0], X_outliers[0][1]),\n             xytext=(2.5, 1.7),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])\n\nplt.sca(axes[1])\nplt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")\nplt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")\nplot_svc_decision_boundary(svm_clf2, 0, 5.5)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[1][0], X_outliers[1][1]),\n             xytext=(3.2, 0.08),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])\n\nsave_fig(\"sensitivity_to_outliers_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is the first code example in chapter 5:**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n\nsvm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n    ])\n\nsvm_clf.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_clf.predict([[5.5, 1.7]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate Figure 5–4. Large margin versus fewer margin violations**","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nsvm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42)\nsvm_clf2 = LinearSVC(C=100, loss=\"hinge\", random_state=42)\n\nscaled_svm_clf1 = Pipeline([\n        (\"scaler\", scaler),\n        (\"linear_svc\", svm_clf1),\n    ])\nscaled_svm_clf2 = Pipeline([\n        (\"scaler\", scaler),\n        (\"linear_svc\", svm_clf2),\n    ])\n\nscaled_svm_clf1.fit(X, y)\nscaled_svm_clf2.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to unscaled parameters\nb1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\nb2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\nw1 = svm_clf1.coef_[0] / scaler.scale_\nw2 = svm_clf2.coef_[0] / scaler.scale_\nsvm_clf1.intercept_ = np.array([b1])\nsvm_clf2.intercept_ = np.array([b2])\nsvm_clf1.coef_ = np.array([w1])\nsvm_clf2.coef_ = np.array([w2])\n\n# Find support vectors (LinearSVC does not do this automatically)\nt = y * 2 - 1\nsupport_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\nsupport_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\nsvm_clf1.support_vectors_ = X[support_vectors_idx1]\nsvm_clf2.support_vectors_ = X[support_vectors_idx2]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\nplot_svc_decision_boundary(svm_clf1, 4, 5.9)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.title(\"$C = {}$\".format(svm_clf1.C), fontsize=16)\nplt.axis([4, 5.9, 0.8, 2.8])\n\nplt.sca(axes[1])\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\nplot_svc_decision_boundary(svm_clf2, 4, 5.99)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.title(\"$C = {}$\".format(svm_clf2.C), fontsize=16)\nplt.axis([4, 5.9, 0.8, 2.8])\n\nsave_fig(\"regularization_plot\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Nonlinear SVM Classification","metadata":{}},{"cell_type":"markdown","source":"**Code to generate Figure 5–5. Adding features to make a dataset linearly separable**","metadata":{}},{"cell_type":"code","source":"X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\nX2D = np.c_[X1D, X1D**2]\ny = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n\nplt.figure(figsize=(10, 3))\n\nplt.subplot(121)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.plot(X1D[:, 0][y==0], np.zeros(4), \"bs\")\nplt.plot(X1D[:, 0][y==1], np.zeros(5), \"g^\")\nplt.gca().get_yaxis().set_ticks([])\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.axis([-4.5, 4.5, -0.2, 0.2])\n\nplt.subplot(122)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], \"bs\")\nplt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], \"g^\")\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.ylabel(r\"$x_2$  \", fontsize=20, rotation=0)\nplt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\nplt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\nplt.axis([-4.5, 4.5, -1, 17])\n\nplt.subplots_adjust(right=1)\n\nsave_fig(\"higher_dimensions_plot\", tight_layout=False)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1D","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n\ndef plot_dataset(X, y, axes):\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n    plt.axis(axes)\n    plt.grid(True, which='both')\n    plt.xlabel(r\"$x_1$\", fontsize=20)\n    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here is second code example in the chapter:**","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_svm_clf = Pipeline([\n        (\"poly_features\", PolynomialFeatures(degree=3)),\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n    ])\n\npolynomial_svm_clf.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate Figure 5–6. Linear SVM classifier using polynomial features**","metadata":{}},{"cell_type":"code","source":"def plot_predictions(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid(x0s, x1s)\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict(X).reshape(x0.shape)\n    y_decision = clf.decision_function(X).reshape(x0.shape)\n    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n\nplot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n\nsave_fig(\"moons_polynomial_svc_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x0z = np.linspace(0, 1, 4)\nx1z = np.linspace(-1, 1, 5)\nxm, ym = np.meshgrid(x0z, x1z)\nxm.ravel(), ym.ravel(), np.c_[xm.ravel(), ym.ravel()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Polynomial Kernel","metadata":{}},{"cell_type":"markdown","source":"**Next code example:**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\npoly_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n    ])\npoly_kernel_svm_clf.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate Figure 5–7. SVM classifiers with a polynomial kernel**","metadata":{}},{"cell_type":"code","source":"poly100_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n    ])\npoly100_kernel_svm_clf.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n\nplt.sca(axes[0])\nplot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\nplt.title(r\"$d=3, r=1, C=5$\", fontsize=18)\n\nplt.sca(axes[1])\nplot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\nplt.title(r\"$d=10, r=100, C=5$\", fontsize=18)\nplt.ylabel(\"\")\n\nsave_fig(\"moons_kernelized_polynomial_svc_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similarity Features","metadata":{}},{"cell_type":"markdown","source":"**Code to generate Figure 5–8. Similarity features using the Gaussian RBF**","metadata":{}},{"cell_type":"code","source":"def gaussian_rbf(x, landmark, gamma):\n    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n\ngamma = 0.3\n\nx1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\nx2s = gaussian_rbf(x1s, -2, gamma)\nx3s = gaussian_rbf(x1s, 1, gamma)\n\nXK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]\nyk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n\nplt.figure(figsize=(10.5, 4))\n\nplt.subplot(121)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c=\"red\")\nplt.plot(X1D[:, 0][yk==0], np.zeros(4), \"bs\")\nplt.plot(X1D[:, 0][yk==1], np.zeros(5), \"g^\")\nplt.plot(x1s, x2s, \"g--\")\nplt.plot(x1s, x3s, \"b:\")\nplt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.ylabel(r\"Similarity\", fontsize=14)\nplt.annotate(r'$\\mathbf{x}$',\n             xy=(X1D[3, 0], 0),\n             xytext=(-0.5, 0.20),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=18,\n            )\nplt.text(-2, 0.9, \"$x_2$\", ha=\"center\", fontsize=20)\nplt.text(1, 0.9, \"$x_3$\", ha=\"center\", fontsize=20)\nplt.axis([-4.5, 4.5, -0.1, 1.1])\n\nplt.subplot(122)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], \"bs\")\nplt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], \"g^\")\nplt.xlabel(r\"$x_2$\", fontsize=20)\nplt.ylabel(r\"$x_3$  \", fontsize=20, rotation=0)\nplt.annotate(r'$\\phi\\left(\\mathbf{x}\\right)$',\n             xy=(XK[3, 0], XK[3, 1]),\n             xytext=(0.65, 0.50),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=18,\n            )\nplt.plot([-0.1, 1.1], [0.57, -0.1], \"r--\", linewidth=3)\nplt.axis([-0.1, 1.1, -0.1, 1.1])\n    \nplt.subplots_adjust(right=1)\n\nsave_fig(\"kernel_method_plot\")\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1_example = X1D[3, 0]\nfor landmark in (-2, 1):\n    k = gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma)\n    print(\"Phi({}, {}) = {}\".format(x1_example, landmark, k))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian RBF Kernel","metadata":{}},{"cell_type":"markdown","source":"**Next code example:**","metadata":{}},{"cell_type":"code","source":"rbf_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n    ])\nrbf_kernel_svm_clf.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate Figure 5–9. SVM classifiers using an RBF kernel**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\ngamma1, gamma2 = 0.1, 5\nC1, C2 = 0.001, 1000\nhyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n\nsvm_clfs = []\nfor gamma, C in hyperparams:\n    rbf_kernel_svm_clf = Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n        ])\n    rbf_kernel_svm_clf.fit(X, y)\n    svm_clfs.append(rbf_kernel_svm_clf)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n\nfor i, svm_clf in enumerate(svm_clfs):\n    plt.sca(axes[i // 2, i % 2])\n    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n    gamma, C = hyperparams[i]\n    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n    if i in (0, 1):\n        plt.xlabel(\"\")\n    if i in (1, 3):\n        plt.ylabel(\"\")\n\nsave_fig(\"moons_rbf_svc_plot\")\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM Regression","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nm = 50\nX = 2 * np.random.rand(m, 1)\ny = (4 + 3 * X + np.random.randn(m, 1)).ravel()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Next code example:**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR(epsilon=1.5, random_state=42)\nsvm_reg.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate Figure 5–10. SVM Regression**","metadata":{}},{"cell_type":"code","source":"svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)\nsvm_reg2 = LinearSVR(epsilon=0.5, random_state=42)\nsvm_reg1.fit(X, y)\nsvm_reg2.fit(X, y)\n\ndef find_support_vectors(svm_reg, X, y):\n    y_pred = svm_reg.predict(X)\n    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)\n    return np.argwhere(off_margin)\n\nsvm_reg1.support_ = find_support_vectors(svm_reg1, X, y)\nsvm_reg2.support_ = find_support_vectors(svm_reg2, X, y)\n\neps_x1 = 1\neps_y_pred = svm_reg1.predict([[eps_x1]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_svm_regression(svm_reg, X, y, axes):\n    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n    y_pred = svm_reg.predict(x1s)\n    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n    plt.plot(X, y, \"bo\")\n    plt.xlabel(r\"$x_1$\", fontsize=18)\n    plt.legend(loc=\"upper left\", fontsize=18)\n    plt.axis(axes)\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\nplt.sca(axes[0])\nplot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])\nplt.title(r\"$\\epsilon = {}$\".format(svm_reg1.epsilon), fontsize=18)\nplt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n#plt.plot([eps_x1, eps_x1], [eps_y_pred, eps_y_pred - svm_reg1.epsilon], \"k-\", linewidth=2)\nplt.annotate(\n        '', xy=(eps_x1, eps_y_pred), xycoords='data',\n        xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),\n        textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}\n    )\nplt.text(0.91, 5.6, r\"$\\epsilon$\", fontsize=20)\nplt.sca(axes[1])\nplot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])\nplt.title(r\"$\\epsilon = {}$\".format(svm_reg2.epsilon), fontsize=18)\nsave_fig(\"svm_regression_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nm = 100\nX = 2 * np.random.rand(m, 1) - 1\ny = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note**: to be future-proof, we set `gamma=\"scale\"`, as this will be the default value in Scikit-Learn 0.22.","metadata":{}},{"cell_type":"markdown","source":"**Next code example:**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\nsvm_poly_reg.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate Figure 5–11. SVM Regression using a second-degree polynomial kernel**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvm_poly_reg1 = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\nsvm_poly_reg2 = SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1, gamma=\"scale\")\nsvm_poly_reg1.fit(X, y)\nsvm_poly_reg2.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\nplt.sca(axes[0])\nplot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\nplt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18)\nplt.ylabel(r\"$y$\", fontsize=18, rotation=0)\nplt.sca(axes[1])\nplot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\nplt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18)\nsave_fig(\"svm_with_polynomial_kernel_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Under the Hood\n## Decision Function and Predictions","metadata":{}},{"cell_type":"markdown","source":"**Code to generate Figure 5–12. Decision function for the iris dataset**","metadata":{}},{"cell_type":"code","source":"iris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3D_decision_function(ax, w, b, x1_lim=[4, 6], x2_lim=[0.8, 2.8]):\n    x1_in_bounds = (X[:, 0] > x1_lim[0]) & (X[:, 0] < x1_lim[1])\n    X_crop = X[x1_in_bounds]\n    y_crop = y[x1_in_bounds]\n    x1s = np.linspace(x1_lim[0], x1_lim[1], 20)\n    x2s = np.linspace(x2_lim[0], x2_lim[1], 20)\n    x1, x2 = np.meshgrid(x1s, x2s)\n    xs = np.c_[x1.ravel(), x2.ravel()]\n    df = (xs.dot(w) + b).reshape(x1.shape)\n    m = 1 / np.linalg.norm(w)\n    boundary_x2s = -x1s*(w[0]/w[1])-b/w[1]\n    margin_x2s_1 = -x1s*(w[0]/w[1])-(b-1)/w[1]\n    margin_x2s_2 = -x1s*(w[0]/w[1])-(b+1)/w[1]\n    ax.plot_surface(x1s, x2, np.zeros_like(x1),\n                    color=\"b\", alpha=0.2, cstride=100, rstride=100)\n    ax.plot(x1s, boundary_x2s, 0, \"k-\", linewidth=2, label=r\"$h=0$\")\n    ax.plot(x1s, margin_x2s_1, 0, \"k--\", linewidth=2, label=r\"$h=\\pm 1$\")\n    ax.plot(x1s, margin_x2s_2, 0, \"k--\", linewidth=2)\n    ax.plot(X_crop[:, 0][y_crop==1], X_crop[:, 1][y_crop==1], 0, \"g^\")\n    ax.plot_wireframe(x1, x2, df, alpha=0.3, color=\"k\")\n    ax.plot(X_crop[:, 0][y_crop==0], X_crop[:, 1][y_crop==0], 0, \"bs\")\n    ax.axis(x1_lim + x2_lim)\n    ax.text(4.5, 2.5, 3.8, \"Decision function $h$\", fontsize=16)\n    ax.set_xlabel(r\"Petal length\", fontsize=16, labelpad=10)\n    ax.set_ylabel(r\"Petal width\", fontsize=16, labelpad=10)\n    ax.set_zlabel(r\"$h = \\mathbf{w}^T \\mathbf{x} + b$\", fontsize=18, labelpad=5)\n    ax.legend(loc=\"upper left\", fontsize=16)\n\nfig = plt.figure(figsize=(11, 6))\nax1 = fig.add_subplot(111, projection='3d')\nplot_3D_decision_function(ax1, w=svm_clf2.coef_[0], b=svm_clf2.intercept_[0])\n\nsave_fig(\"iris_3D_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate Figure 5–13. A smaller weight vector results in a larger margin**","metadata":{}},{"cell_type":"code","source":"def plot_2D_decision_function(w, b, ylabel=True, x1_lim=[-3, 3]):\n    x1 = np.linspace(x1_lim[0], x1_lim[1], 200)\n    y = w * x1 + b\n    m = 1 / w\n\n    plt.plot(x1, y)\n    plt.plot(x1_lim, [1, 1], \"k:\")\n    plt.plot(x1_lim, [-1, -1], \"k:\")\n    plt.axhline(y=0, color='k')\n    plt.axvline(x=0, color='k')\n    plt.plot([m, m], [0, 1], \"k--\")\n    plt.plot([-m, -m], [0, -1], \"k--\")\n    plt.plot([-m, m], [0, 0], \"k-o\", linewidth=3)\n    plt.axis(x1_lim + [-2, 2])\n    plt.xlabel(r\"$x_1$\", fontsize=16)\n    if ylabel:\n        plt.ylabel(r\"$w_1 x_1$  \", rotation=0, fontsize=16)\n    plt.title(r\"$w_1 = {}$\".format(w), fontsize=16)\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 3.2), sharey=True)\nplt.sca(axes[0])\nplot_2D_decision_function(1, 0)\nplt.sca(axes[1])\nplot_2D_decision_function(0.5, 0, ylabel=False)\nsave_fig(\"small_w_large_margin_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)] # petal length, petal width\ny = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n\nsvm_clf = SVC(kernel=\"linear\", C=1)\nsvm_clf.fit(X, y)\nsvm_clf.predict([[5.3, 1.3]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to generate the Hinge Loss figure:**","metadata":{}},{"cell_type":"code","source":"t = np.linspace(-2, 4, 200)\nh = np.where(1 - t < 0, 0, 1 - t)  # max(0, 1-t)\n\nplt.figure(figsize=(5,2.8))\nplt.plot(t, h, \"b-\", linewidth=2, label=\"$max(0, 1 - t)$\")\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.yticks(np.arange(-1, 2.5, 1))\nplt.xlabel(\"$t$\", fontsize=16)\nplt.axis([-2, 4, -1, 2.5])\nplt.legend(loc=\"upper right\", fontsize=16)\nsave_fig(\"hinge_plot\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extra material","metadata":{}},{"cell_type":"markdown","source":"## Training time","metadata":{}},{"cell_type":"code","source":"X, y = make_moons(n_samples=1000, noise=0.4, random_state=42)\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ntol = 0.1\ntols = []\ntimes = []\nfor i in range(10):\n    svm_clf = SVC(kernel=\"poly\", gamma=3, C=10, tol=tol, verbose=1)\n    t1 = time.time()\n    svm_clf.fit(X, y)\n    t2 = time.time()\n    times.append(t2-t1)\n    tols.append(tol)\n    print(i, tol, t2-t1)\n    tol /= 10\nplt.semilogx(tols, times, \"bo-\")\nplt.xlabel(\"Tolerance\", fontsize=16)\nplt.ylabel(\"Time (seconds)\", fontsize=16)\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear SVM classifier implementation using Batch Gradient Descent","metadata":{}},{"cell_type":"code","source":"# Training set\nX = iris[\"data\"][:, (2, 3)] # petal length, petal width\ny = (iris[\"target\"] == 2).astype(np.float64).reshape(-1, 1) # Iris virginica","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator\n\nclass MyLinearSVC(BaseEstimator):\n    def __init__(self, C=1, eta0=1, eta_d=10000, n_epochs=1000, random_state=None):\n        self.C = C\n        self.eta0 = eta0\n        self.n_epochs = n_epochs\n        self.random_state = random_state\n        self.eta_d = eta_d\n\n    def eta(self, epoch):\n        return self.eta0 / (epoch + self.eta_d)\n        \n    def fit(self, X, y):\n        # Random initialization\n        if self.random_state:\n            np.random.seed(self.random_state)\n        w = np.random.randn(X.shape[1], 1) # n feature weights\n        b = 0\n\n        m = len(X)\n        t = y * 2 - 1  # -1 if y==0, +1 if y==1\n        X_t = X * t\n        self.Js=[]\n\n        # Training\n        for epoch in range(self.n_epochs):\n            support_vectors_idx = (X_t.dot(w) + t * b < 1).ravel()\n            X_t_sv = X_t[support_vectors_idx]\n            t_sv = t[support_vectors_idx]\n\n            J = 1/2 * np.sum(w * w) + self.C * (np.sum(1 - X_t_sv.dot(w)) - b * np.sum(t_sv))\n            self.Js.append(J)\n\n            w_gradient_vector = w - self.C * np.sum(X_t_sv, axis=0).reshape(-1, 1)\n            b_derivative = -self.C * np.sum(t_sv)\n                \n            w = w - self.eta(epoch) * w_gradient_vector\n            b = b - self.eta(epoch) * b_derivative\n            \n\n        self.intercept_ = np.array([b])\n        self.coef_ = np.array([w])\n        support_vectors_idx = (X_t.dot(w) + t * b < 1).ravel()\n        self.support_vectors_ = X[support_vectors_idx]\n        return self\n\n    def decision_function(self, X):\n        return X.dot(self.coef_[0]) + self.intercept_[0]\n\n    def predict(self, X):\n        return (self.decision_function(X) >= 0).astype(np.float64)\n\nC=2\nsvm_clf = MyLinearSVC(C=C, eta0 = 10, eta_d = 1000, n_epochs=60000, random_state=2)\nsvm_clf.fit(X, y)\nsvm_clf.predict(np.array([[5, 2], [4, 1]]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(svm_clf.n_epochs), svm_clf.Js)\nplt.axis([0, svm_clf.n_epochs, 0, 100])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(svm_clf.intercept_, svm_clf.coef_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_clf2 = SVC(kernel=\"linear\", C=C)\nsvm_clf2.fit(X, y.ravel())\nprint(svm_clf2.intercept_, svm_clf2.coef_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yr = y.ravel()\nfig, axes = plt.subplots(ncols=2, figsize=(11, 3.2), sharey=True)\nplt.sca(axes[0])\nplt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\", label=\"Not Iris virginica\")\nplot_svc_decision_boundary(svm_clf, 4, 6)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.title(\"MyLinearSVC\", fontsize=14)\nplt.axis([4, 6, 0.8, 2.8])\nplt.legend(loc=\"upper left\")\n\nplt.sca(axes[1])\nplt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\")\nplt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\")\nplot_svc_decision_boundary(svm_clf2, 4, 6)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.title(\"SVC\", fontsize=14)\nplt.axis([4, 6, 0.8, 2.8])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss=\"hinge\", alpha=0.017, max_iter=1000, tol=1e-3, random_state=42)\nsgd_clf.fit(X, y.ravel())\n\nm = len(X)\nt = y * 2 - 1  # -1 if y==0, +1 if y==1\nX_b = np.c_[np.ones((m, 1)), X]  # Add bias input x0=1\nX_b_t = X_b * t\nsgd_theta = np.r_[sgd_clf.intercept_[0], sgd_clf.coef_[0]]\nprint(sgd_theta)\nsupport_vectors_idx = (X_b_t.dot(sgd_theta) < 1).ravel()\nsgd_clf.support_vectors_ = X[support_vectors_idx]\nsgd_clf.C = C\n\nplt.figure(figsize=(5.5,3.2))\nplt.plot(X[:, 0][yr==1], X[:, 1][yr==1], \"g^\")\nplt.plot(X[:, 0][yr==0], X[:, 1][yr==0], \"bs\")\nplot_svc_decision_boundary(sgd_clf, 4, 6)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.title(\"SGDClassifier\", fontsize=14)\nplt.axis([4, 6, 0.8, 2.8])\n","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Own solutions to exercises","metadata":{}},{"cell_type":"markdown","source":"## Exercise 1\n\nA boundary function is defined that partitions the space into as many parts as the classes. A sample is given to this boundary function and it determines the class.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 2\n\nThe support vectors are the training samples closest to the boundary.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 3\n\nIf there are widely different scales, the decision boundary becomes very small in the dimensions where the range is small.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 4\n\nNo probability, but confidence maybe based on the distance from the boundary.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 5\n\nThe primal form should be used, since it is faster when the number if instances is larger than the number of features.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 6\n\nBoth should be increased.","metadata":{}},{"cell_type":"markdown","source":"## Exercise 7\n\nn_p = 2\n\nH = [[0, 1],\n    [1, 0]]\n    \nf = [0, 0]\n\nb = [-1...]","metadata":{}},{"cell_type":"markdown","source":"## Exercise 8","metadata":{}},{"cell_type":"code","source":"iris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\ny_01_idx = (y==0) | (y==1)\nX01 = X[y_01_idx]\ny01 = y[y_01_idx]\n\nnum = len(y01)\na=np.arange(num)\nnp.random.shuffle(a)\n\ncut = num*8//10\n\nscaler = StandardScaler()\n\nX01_train = scaler.fit_transform(X[a[:cut]])\nX01_validate = scaler.transform(X[a[cut:]])\ny01_train = y[a[:cut]]\ny01_validate = y[a[cut:]]\n\nC = 5\nalpha = 1 / (C * len(X))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lsvc = LinearSVC(loss = \"hinge\", C=C, random_state = 42)\nlsvc.fit(X01_train, y01_train)\nlsvc.coef_, lsvc.intercept_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny01_predict = lsvc.predict(X01_validate)\naccuracy_score(y01_validate, y01_predict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(kernel=\"linear\", C=C)\nsvc.fit(X01_train, y01_train)\nsvc.coef_, svc.intercept_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier(loss = \"hinge\", learning_rate=\"constant\", eta0=0.001, alpha=alpha,\n                    max_iter=1000, tol=1e-3, random_state=42)\nsgd.fit(X01_train, y01_train)\nsgd.coef_, sgd.intercept_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exercise 9","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX = mnist[\"data\"]\ny = mnist[\"target\"].astype(np.uint8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\n\nC=5\n\nsvm0 = SVC(kernel = \"linear\", C=C, verbose=True)\ny_train0 = np.zeros((len(y_train), 2))\ny_train0[y_train==0, 0]=1\ny_train0[y_train!=0, 1]=1\nsvm0.fit(X_train, y_train)\n#clf = OneVsRestClassifier(svm, n_jobs=3).fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import reciprocal, uniform\n\nreciprocal(0.001, 0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exercise solutions","metadata":{}},{"cell_type":"markdown","source":"## 1. to 7.","metadata":{}},{"cell_type":"markdown","source":"See appendix A.","metadata":{}},{"cell_type":"markdown","source":"# 8.","metadata":{}},{"cell_type":"markdown","source":"_Exercise: train a `LinearSVC` on a linearly separable dataset. Then train an `SVC` and a `SGDClassifier` on the same dataset. See if you can get them to produce roughly the same model._","metadata":{}},{"cell_type":"markdown","source":"Let's use the Iris dataset: the Iris Setosa and Iris Versicolor classes are linearly separable.","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nC = 5\nalpha = 1 / (C * len(X))\n\nlin_clf = LinearSVC(loss=\"hinge\", C=C, random_state=42)\nsvm_clf = SVC(kernel=\"linear\", C=C)\nsgd_clf = SGDClassifier(loss=\"hinge\", learning_rate=\"constant\", eta0=0.001, alpha=alpha,\n                        max_iter=1000, tol=1e-3, random_state=42)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nlin_clf.fit(X_scaled, y)\nsvm_clf.fit(X_scaled, y)\nsgd_clf.fit(X_scaled, y)\n\nprint(\"LinearSVC:                   \", lin_clf.intercept_, lin_clf.coef_)\nprint(\"SVC:                         \", svm_clf.intercept_, svm_clf.coef_)\nprint(\"SGDClassifier(alpha={:.5f}):\".format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the decision boundaries of these three models:","metadata":{}},{"cell_type":"code","source":"# Compute the slope and bias of each decision boundary\nw1 = -lin_clf.coef_[0, 0]/lin_clf.coef_[0, 1]\nb1 = -lin_clf.intercept_[0]/lin_clf.coef_[0, 1]\nw2 = -svm_clf.coef_[0, 0]/svm_clf.coef_[0, 1]\nb2 = -svm_clf.intercept_[0]/svm_clf.coef_[0, 1]\nw3 = -sgd_clf.coef_[0, 0]/sgd_clf.coef_[0, 1]\nb3 = -sgd_clf.intercept_[0]/sgd_clf.coef_[0, 1]\n\n# Transform the decision boundary lines back to the original scale\nline1 = scaler.inverse_transform([[-10, -10 * w1 + b1], [10, 10 * w1 + b1]])\nline2 = scaler.inverse_transform([[-10, -10 * w2 + b2], [10, 10 * w2 + b2]])\nline3 = scaler.inverse_transform([[-10, -10 * w3 + b3], [10, 10 * w3 + b3]])\n\n# Plot all three decision boundaries\nplt.figure(figsize=(11, 4))\nplt.plot(line1[:, 0], line1[:, 1], \"k:\", label=\"LinearSVC\")\nplt.plot(line2[:, 0], line2[:, 1], \"b--\", linewidth=2, label=\"SVC\")\nplt.plot(line3[:, 0], line3[:, 1], \"r-\", label=\"SGDClassifier\")\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\") # label=\"Iris versicolor\"\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\") # label=\"Iris setosa\"\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper center\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Close enough!","metadata":{}},{"cell_type":"markdown","source":"# 9.","metadata":{}},{"cell_type":"markdown","source":"_Exercise: train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?_","metadata":{}},{"cell_type":"markdown","source":"First, let's load the dataset and split it into a training set and a test set. We could use `train_test_split()` but people usually just take the first 60,000 instances for the training set, and the last 10,000 instances for the test set (this makes it possible to compare your model's performance with others): ","metadata":{}},{"cell_type":"markdown","source":"**Warning:** since Scikit-Learn 0.24, `fetch_openml()` returns a Pandas `DataFrame` by default. To avoid this, we use `as_frame=False`.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n\nX = mnist[\"data\"]\ny = mnist[\"target\"].astype(np.uint8)\n\nX_train = X[:60000]\ny_train = y[:60000]\nX_test = X[60000:]\ny_test = y[60000:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Many training algorithms are sensitive to the order of the training instances, so it's generally good practice to shuffle them first. However, the dataset is already shuffled, so we do not need to do it.","metadata":{}},{"cell_type":"markdown","source":"Let's start simple, with a linear SVM classifier. It will automatically use the One-vs-All (also called One-vs-the-Rest, OvR) strategy, so there's nothing special we need to do. Easy!\n\n**Warning**: this may take a few minutes depending on your hardware.","metadata":{}},{"cell_type":"code","source":"lin_clf = LinearSVC(random_state=42)\nlin_clf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make predictions on the training set and measure the accuracy (we don't want to measure it on the test set yet, since we have not selected and trained the final model yet):","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred = lin_clf.predict(X_train)\naccuracy_score(y_train, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, 83.5% accuracy on MNIST is pretty bad. This linear model is certainly too simple for MNIST, but perhaps we just needed to scale the data first:","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float32))\nX_test_scaled = scaler.transform(X_test.astype(np.float32))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Warning**: this may take a few minutes depending on your hardware.","metadata":{}},{"cell_type":"code","source":"lin_clf = LinearSVC(random_state=42)\nlin_clf.fit(X_train_scaled, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = lin_clf.predict(X_train_scaled)\naccuracy_score(y_train, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's much better (we cut the error rate by about 53%), but still not great at all for MNIST. If we want to use an SVM, we will have to use a kernel. Let's try an `SVC` with an RBF kernel (the default).","metadata":{}},{"cell_type":"markdown","source":"**Note**: to be future-proof we set `gamma=\"scale\"` since it will be the default value in Scikit-Learn 0.22.","metadata":{}},{"cell_type":"code","source":"svm_clf = SVC(gamma=\"scale\")\nsvm_clf.fit(X_train_scaled[:10000], y_train[:10000])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = svm_clf.predict(X_train_scaled)\naccuracy_score(y_train, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's promising, we get better performance even though we trained the model on 6 times less data. Let's tune the hyperparameters by doing a randomized search with cross validation. We will do this on a small dataset just to speed up the process:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nrnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2, cv=3)\nrnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnd_search_cv.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnd_search_cv.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This looks pretty low but remember we only trained the model on 1,000 instances. Let's retrain the best estimator on the whole training set:","metadata":{}},{"cell_type":"markdown","source":"**Warning**: the following cell may take hours to run, depending on your hardware.","metadata":{}},{"cell_type":"code","source":"rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\naccuracy_score(y_train, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ah, this looks good! Let's select this model. Now we can test it on the test set:","metadata":{}},{"cell_type":"code","source":"y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\naccuracy_score(y_test, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not too bad, but apparently the model is overfitting slightly. It's tempting to tweak the hyperparameters a bit more (e.g. decreasing `C` and/or `gamma`), but we would run the risk of overfitting the test set. Other people have found that the hyperparameters `C=5` and `gamma=0.005` yield even better performance (over 98% accuracy). By running the randomized search for longer and on a larger part of the training set, you may be able to find this as well.","metadata":{}},{"cell_type":"markdown","source":"## 10.","metadata":{}},{"cell_type":"markdown","source":"_Exercise: train an SVM regressor on the California housing dataset._","metadata":{}},{"cell_type":"markdown","source":"Let's load the dataset using Scikit-Learn's `fetch_california_housing()` function:","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_california_housing\n\nhousing = fetch_california_housing()\nX = housing[\"data\"]\ny = housing[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:22:32.827017Z","iopub.execute_input":"2022-03-18T18:22:32.827430Z","iopub.status.idle":"2022-03-18T18:22:33.981944Z","shell.execute_reply.started":"2022-03-18T18:22:32.827390Z","shell.execute_reply":"2022-03-18T18:22:33.980974Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Split it into a training set and a test set:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:22:34.503746Z","iopub.execute_input":"2022-03-18T18:22:34.504557Z","iopub.status.idle":"2022-03-18T18:22:34.567447Z","shell.execute_reply.started":"2022-03-18T18:22:34.504503Z","shell.execute_reply":"2022-03-18T18:22:34.566567Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Don't forget to scale the data:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:22:36.682297Z","iopub.execute_input":"2022-03-18T18:22:36.683063Z","iopub.status.idle":"2022-03-18T18:22:36.692295Z","shell.execute_reply.started":"2022-03-18T18:22:36.683023Z","shell.execute_reply":"2022-03-18T18:22:36.691181Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Let's train a simple `LinearSVR` first:","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVR\n\nlin_svr = LinearSVR(random_state=42)\nlin_svr.fit(X_train_scaled, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:22:44.623405Z","iopub.execute_input":"2022-03-18T18:22:44.623730Z","iopub.status.idle":"2022-03-18T18:22:45.839463Z","shell.execute_reply.started":"2022-03-18T18:22:44.623696Z","shell.execute_reply":"2022-03-18T18:22:45.838460Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"LinearSVR(random_state=42)"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's see how it performs on the training set:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ny_pred = lin_svr.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nmse","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:22:47.809387Z","iopub.execute_input":"2022-03-18T18:22:47.810136Z","iopub.status.idle":"2022-03-18T18:22:47.821540Z","shell.execute_reply.started":"2022-03-18T18:22:47.810085Z","shell.execute_reply":"2022-03-18T18:22:47.820564Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0.9641780189948642"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's look at the RMSE:","metadata":{}},{"cell_type":"code","source":"np.sqrt(mse)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:22:50.237529Z","iopub.execute_input":"2022-03-18T18:22:50.237916Z","iopub.status.idle":"2022-03-18T18:22:50.244869Z","shell.execute_reply.started":"2022-03-18T18:22:50.237878Z","shell.execute_reply":"2022-03-18T18:22:50.243992Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0.9819256687727764"},"metadata":{}}]},{"cell_type":"markdown","source":"In this training set, the targets are tens of thousands of dollars. The RMSE gives a rough idea of the kind of error you should expect (with a higher weight for large errors): so with this model we can expect errors somewhere around $10,000. Not great. Let's see if we can do better with an RBF Kernel. We will use randomized search with cross validation to find the appropriate hyperparameter values for `C` and `gamma`:","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nrnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42)\nrnd_search_cv.fit(X_train_scaled, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:22:52.308400Z","iopub.execute_input":"2022-03-18T18:22:52.308728Z","iopub.status.idle":"2022-03-18T18:27:25.238479Z","shell.execute_reply.started":"2022-03-18T18:22:52.308695Z","shell.execute_reply":"2022-03-18T18:27:25.237287Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] END .....C=4.745401188473625, gamma=0.07969454818643928; total time=   9.5s\n[CV] END .....C=4.745401188473625, gamma=0.07969454818643928; total time=   9.6s\n[CV] END .....C=4.745401188473625, gamma=0.07969454818643928; total time=   9.5s\n[CV] END .....C=8.31993941811405, gamma=0.015751320499779724; total time=   8.9s\n[CV] END .....C=8.31993941811405, gamma=0.015751320499779724; total time=   8.9s\n[CV] END .....C=8.31993941811405, gamma=0.015751320499779724; total time=   8.9s\n[CV] END ....C=2.560186404424365, gamma=0.002051110418843397; total time=   8.3s\n[CV] END ....C=2.560186404424365, gamma=0.002051110418843397; total time=   8.3s\n[CV] END ....C=2.560186404424365, gamma=0.002051110418843397; total time=   8.4s\n[CV] END ....C=1.5808361216819946, gamma=0.05399484409787431; total time=   8.3s\n[CV] END ....C=1.5808361216819946, gamma=0.05399484409787431; total time=   8.2s\n[CV] END ....C=1.5808361216819946, gamma=0.05399484409787431; total time=   8.3s\n[CV] END ....C=7.011150117432088, gamma=0.026070247583707663; total time=   9.2s\n[CV] END ....C=7.011150117432088, gamma=0.026070247583707663; total time=   9.1s\n[CV] END ....C=7.011150117432088, gamma=0.026070247583707663; total time=   9.1s\n[CV] END .....C=1.2058449429580245, gamma=0.0870602087830485; total time=   8.1s\n[CV] END .....C=1.2058449429580245, gamma=0.0870602087830485; total time=   8.3s\n[CV] END .....C=1.2058449429580245, gamma=0.0870602087830485; total time=   8.1s\n[CV] END ...C=9.324426408004218, gamma=0.0026587543983272693; total time=   8.5s\n[CV] END ...C=9.324426408004218, gamma=0.0026587543983272693; total time=   8.5s\n[CV] END ...C=9.324426408004218, gamma=0.0026587543983272693; total time=   8.4s\n[CV] END ...C=2.818249672071006, gamma=0.0023270677083837795; total time=   8.3s\n[CV] END ...C=2.818249672071006, gamma=0.0023270677083837795; total time=   8.4s\n[CV] END ...C=2.818249672071006, gamma=0.0023270677083837795; total time=   8.4s\n[CV] END ....C=4.042422429595377, gamma=0.011207606211860567; total time=   8.3s\n[CV] END ....C=4.042422429595377, gamma=0.011207606211860567; total time=   8.4s\n[CV] END ....C=4.042422429595377, gamma=0.011207606211860567; total time=   8.4s\n[CV] END ....C=5.319450186421157, gamma=0.003823475224675185; total time=   8.4s\n[CV] END ....C=5.319450186421157, gamma=0.003823475224675185; total time=   8.5s\n[CV] END ....C=5.319450186421157, gamma=0.003823475224675185; total time=   8.5s\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=3, estimator=SVR(),\n                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f659bb51310>,\n                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f659bb47f50>},\n                   random_state=42, verbose=2)"},"metadata":{}}]},{"cell_type":"code","source":"rnd_search_cv.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:27:34.820566Z","iopub.execute_input":"2022-03-18T18:27:34.820916Z","iopub.status.idle":"2022-03-18T18:27:34.828566Z","shell.execute_reply.started":"2022-03-18T18:27:34.820877Z","shell.execute_reply":"2022-03-18T18:27:34.827618Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"SVR(C=4.745401188473625, gamma=0.07969454818643928)"},"metadata":{}}]},{"cell_type":"markdown","source":"Now let's measure the RMSE on the training set:","metadata":{}},{"cell_type":"code","source":"y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nnp.sqrt(mse)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:27:36.665790Z","iopub.execute_input":"2022-03-18T18:27:36.666442Z","iopub.status.idle":"2022-03-18T18:27:48.092406Z","shell.execute_reply.started":"2022-03-18T18:27:36.666398Z","shell.execute_reply":"2022-03-18T18:27:48.091357Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0.5727524770785356"},"metadata":{}}]},{"cell_type":"markdown","source":"Looks much better than the linear model. Let's select this model and evaluate it on the test set:","metadata":{}},{"cell_type":"code","source":"y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\nmse = mean_squared_error(y_test, y_pred)\nnp.sqrt(mse)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T18:27:56.099634Z","iopub.execute_input":"2022-03-18T18:27:56.100234Z","iopub.status.idle":"2022-03-18T18:27:58.958525Z","shell.execute_reply.started":"2022-03-18T18:27:56.100180Z","shell.execute_reply":"2022-03-18T18:27:58.957549Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0.592916838552874"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}