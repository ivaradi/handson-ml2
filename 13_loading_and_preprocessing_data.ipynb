{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Chapter 13 – Loading and Preprocessing Data with TensorFlow**\n\n_This notebook contains all the sample code and solutions to the exercises in chapter 13._","metadata":{}},{"cell_type":"markdown","source":"<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/13_loading_and_preprocessing_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml2/blob/master/13_loading_and_preprocessing_data.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.","metadata":{}},{"cell_type":"code","source":"# Python ≥3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n                                                                                                                                                                                                                                                                                                \n# Is this notebook running on Colab or Kaggle?\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\n\nif IS_COLAB or IS_KAGGLE:\n    %pip install -q -U tfx\n    print(\"You can safely ignore the package incompatibility errors.\")\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# TensorFlow ≥2.0 is required\nimport tensorflow as tf\nfrom tensorflow import keras\nassert tf.__version__ >= \"2.0\"\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"data\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:33:32.829312Z","iopub.execute_input":"2023-04-25T16:33:32.829727Z","iopub.status.idle":"2023-04-25T16:34:55.410212Z","shell.execute_reply.started":"2023-04-25T16:33:32.829692Z","shell.execute_reply":"2023-04-25T16:34:55.408876Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nstatsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\nshap 0.41.0 requires packaging>20.9, but you have packaging 20.9 which is incompatible.\npytoolconfig 1.2.5 requires packaging>=22.0, but you have packaging 20.9 which is incompatible.\npytesseract 0.3.10 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\npathos 0.3.0 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\nonnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\nmultiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\nlibrosa 0.10.0 requires soundfile>=0.12.1, but you have soundfile 0.11.0 which is incompatible.\ngoogle-cloud-monitoring 2.14.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.33.2, but you have google-api-core 1.32.0 which is incompatible.\nflask 2.2.3 requires click>=8.0, but you have click 7.1.2 which is incompatible.\ncloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.11 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nYou can safely ignore the package incompatibility errors.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Datasets","metadata":{}},{"cell_type":"code","source":"X = tf.range(10)\ndataset = tf.data.Dataset.from_tensor_slices(X)\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:07.026139Z","iopub.execute_input":"2023-03-29T12:52:07.027017Z","iopub.status.idle":"2023-03-29T12:52:07.144189Z","shell.execute_reply.started":"2023-03-29T12:52:07.026969Z","shell.execute_reply":"2023-03-29T12:52:07.142802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Equivalently:","metadata":{}},{"cell_type":"code","source":"dataset = tf.data.Dataset.range(10)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:08.022446Z","iopub.execute_input":"2023-03-29T12:52:08.023802Z","iopub.status.idle":"2023-03-29T12:52:08.035938Z","shell.execute_reply.started":"2023-03-29T12:52:08.023736Z","shell.execute_reply":"2023-03-29T12:52:08.034561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for item in dataset:\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:08.529115Z","iopub.execute_input":"2023-03-29T12:52:08.529544Z","iopub.status.idle":"2023-03-29T12:52:08.644035Z","shell.execute_reply.started":"2023-03-29T12:52:08.529504Z","shell.execute_reply":"2023-03-29T12:52:08.642525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.repeat(3).batch(7)\nfor item in dataset:\n    print(item)","metadata":{"tags":["raises-exception"],"execution":{"iopub.status.busy":"2023-03-29T12:52:09.659208Z","iopub.execute_input":"2023-03-29T12:52:09.660091Z","iopub.status.idle":"2023-03-29T12:52:09.692745Z","shell.execute_reply.started":"2023-03-29T12:52:09.660007Z","shell.execute_reply":"2023-03-29T12:52:09.690914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(lambda x: x * 2)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:09.950762Z","iopub.execute_input":"2023-03-29T12:52:09.951192Z","iopub.status.idle":"2023-03-29T12:52:09.988371Z","shell.execute_reply.started":"2023-03-29T12:52:09.951154Z","shell.execute_reply":"2023-03-29T12:52:09.987071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for item in dataset:\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:10.188910Z","iopub.execute_input":"2023-03-29T12:52:10.189433Z","iopub.status.idle":"2023-03-29T12:52:10.243209Z","shell.execute_reply.started":"2023-03-29T12:52:10.189393Z","shell.execute_reply":"2023-03-29T12:52:10.241702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset = dataset.apply(tf.data.experimental.unbatch()) # Now deprecated\ndataset = dataset.unbatch()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:10.383564Z","iopub.execute_input":"2023-03-29T12:52:10.383987Z","iopub.status.idle":"2023-03-29T12:52:10.402574Z","shell.execute_reply.started":"2023-03-29T12:52:10.383950Z","shell.execute_reply":"2023-03-29T12:52:10.400933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.filter(lambda x: x < 10)  # keep only items < 10","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:10.678310Z","iopub.execute_input":"2023-03-29T12:52:10.678758Z","iopub.status.idle":"2023-03-29T12:52:10.710804Z","shell.execute_reply.started":"2023-03-29T12:52:10.678716Z","shell.execute_reply":"2023-03-29T12:52:10.709390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for item in dataset.take(3):\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:10.964615Z","iopub.execute_input":"2023-03-29T12:52:10.965022Z","iopub.status.idle":"2023-03-29T12:52:11.001862Z","shell.execute_reply.started":"2023-03-29T12:52:10.964987Z","shell.execute_reply":"2023-03-29T12:52:11.000780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\n\ndataset = tf.data.Dataset.range(10).repeat(3)\ndataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\nfor item in dataset:\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:11.336184Z","iopub.execute_input":"2023-03-29T12:52:11.337088Z","iopub.status.idle":"2023-03-29T12:52:11.391048Z","shell.execute_reply.started":"2023-03-29T12:52:11.337040Z","shell.execute_reply":"2023-03-29T12:52:11.389327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the California dataset to multiple CSV files","metadata":{}},{"cell_type":"markdown","source":"Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it:","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nhousing = fetch_california_housing()\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    housing.data, housing.target.reshape(-1, 1), random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train_full, y_train_full, random_state=42)\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_mean = scaler.mean_\nX_std = scaler.scale_","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:12.446202Z","iopub.execute_input":"2023-03-29T12:52:12.446693Z","iopub.status.idle":"2023-03-29T12:52:13.394809Z","shell.execute_reply.started":"2023-03-29T12:52:12.446648Z","shell.execute_reply":"2023-03-29T12:52:13.393315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. To demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV files:","metadata":{}},{"cell_type":"code","source":"def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n    housing_dir = os.path.join(\"datasets\", \"housing\")\n    os.makedirs(housing_dir, exist_ok=True)\n    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n\n    filepaths = []\n    m = len(data)\n    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n        part_csv = path_format.format(name_prefix, file_idx)\n        filepaths.append(part_csv)\n        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n            if header is not None:\n                f.write(header)\n                f.write(\"\\n\")\n            for row_idx in row_indices:\n                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n                f.write(\"\\n\")\n    return filepaths","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:13.397434Z","iopub.execute_input":"2023-03-29T12:52:13.397972Z","iopub.status.idle":"2023-03-29T12:52:13.409115Z","shell.execute_reply.started":"2023-03-29T12:52:13.397908Z","shell.execute_reply":"2023-03-29T12:52:13.407196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = np.c_[X_train, y_train]\nvalid_data = np.c_[X_valid, y_valid]\ntest_data = np.c_[X_test, y_test]\nheader_cols = housing.feature_names + [\"MedianHouseValue\"]\nheader = \",\".join(header_cols)\n\ntrain_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\nvalid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\ntest_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:13.410870Z","iopub.execute_input":"2023-03-29T12:52:13.411552Z","iopub.status.idle":"2023-03-29T12:52:13.577935Z","shell.execute_reply.started":"2023-03-29T12:52:13.411498Z","shell.execute_reply":"2023-03-29T12:52:13.576135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, now let's take a peek at the first few lines of one of these CSV files:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\npd.read_csv(train_filepaths[0]).head()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:13.619863Z","iopub.execute_input":"2023-03-29T12:52:13.620315Z","iopub.status.idle":"2023-03-29T12:52:13.663254Z","shell.execute_reply.started":"2023-03-29T12:52:13.620270Z","shell.execute_reply":"2023-03-29T12:52:13.661710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or in text mode:","metadata":{}},{"cell_type":"code","source":"with open(train_filepaths[0]) as f:\n    for i in range(5):\n        print(f.readline(), end=\"\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:13.955197Z","iopub.execute_input":"2023-03-29T12:52:13.955672Z","iopub.status.idle":"2023-03-29T12:52:13.963970Z","shell.execute_reply.started":"2023-03-29T12:52:13.955628Z","shell.execute_reply":"2023-03-29T12:52:13.962556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_filepaths","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:14.159629Z","iopub.execute_input":"2023-03-29T12:52:14.160106Z","iopub.status.idle":"2023-03-29T12:52:14.168716Z","shell.execute_reply.started":"2023-03-29T12:52:14.160068Z","shell.execute_reply":"2023-03-29T12:52:14.167264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building an Input Pipeline","metadata":{}},{"cell_type":"code","source":"filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:14.500147Z","iopub.execute_input":"2023-03-29T12:52:14.500589Z","iopub.status.idle":"2023-03-29T12:52:14.544221Z","shell.execute_reply.started":"2023-03-29T12:52:14.500547Z","shell.execute_reply":"2023-03-29T12:52:14.543080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for filepath in filepath_dataset:\n    print(filepath)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:14.854207Z","iopub.execute_input":"2023-03-29T12:52:14.854643Z","iopub.status.idle":"2023-03-29T12:52:14.878182Z","shell.execute_reply.started":"2023-03-29T12:52:14.854603Z","shell.execute_reply":"2023-03-29T12:52:14.876815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_readers = 5\ndataset = filepath_dataset.interleave(\n    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n    cycle_length=n_readers)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:15.183275Z","iopub.execute_input":"2023-03-29T12:52:15.184933Z","iopub.status.idle":"2023-03-29T12:52:15.233886Z","shell.execute_reply.started":"2023-03-29T12:52:15.184873Z","shell.execute_reply":"2023-03-29T12:52:15.232401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for line in dataset.take(5):\n    print(line.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:15.552932Z","iopub.execute_input":"2023-03-29T12:52:15.553408Z","iopub.status.idle":"2023-03-29T12:52:15.606808Z","shell.execute_reply.started":"2023-03-29T12:52:15.553363Z","shell.execute_reply":"2023-03-29T12:52:15.605546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that field 4 is interpreted as a string.","metadata":{}},{"cell_type":"code","source":"record_defaults=[0, np.nan, tf.constant(np.nan, dtype=tf.float64), \"Hello\", tf.constant([])]\nparsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\nparsed_fields","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:16.805250Z","iopub.execute_input":"2023-03-29T12:52:16.806211Z","iopub.status.idle":"2023-03-29T12:52:16.825394Z","shell.execute_reply.started":"2023-03-29T12:52:16.806157Z","shell.execute_reply":"2023-03-29T12:52:16.823703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that all missing fields are replaced with their default value, when provided:","metadata":{}},{"cell_type":"code","source":"parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\nparsed_fields","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:17.454345Z","iopub.execute_input":"2023-03-29T12:52:17.454820Z","iopub.status.idle":"2023-03-29T12:52:17.464997Z","shell.execute_reply.started":"2023-03-29T12:52:17.454760Z","shell.execute_reply":"2023-03-29T12:52:17.463634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 5th field is compulsory (since we provided `tf.constant([])` as the \"default value\"), so we get an exception if we do not provide it:","metadata":{}},{"cell_type":"code","source":"try:\n    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\nexcept tf.errors.InvalidArgumentError as ex:\n    print(ex)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:18.364095Z","iopub.execute_input":"2023-03-29T12:52:18.364541Z","iopub.status.idle":"2023-03-29T12:52:18.373827Z","shell.execute_reply.started":"2023-03-29T12:52:18.364502Z","shell.execute_reply":"2023-03-29T12:52:18.372148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of fields should match exactly the number of fields in the `record_defaults`:","metadata":{}},{"cell_type":"code","source":"try:\n    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\nexcept tf.errors.InvalidArgumentError as ex:\n    print(ex)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:19.953215Z","iopub.execute_input":"2023-03-29T12:52:19.953680Z","iopub.status.idle":"2023-03-29T12:52:19.960952Z","shell.execute_reply.started":"2023-03-29T12:52:19.953637Z","shell.execute_reply":"2023-03-29T12:52:19.959657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_inputs = 8 # X_train.shape[-1]\n\n@tf.function\ndef preprocess(line):\n    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n    fields = tf.io.decode_csv(line, record_defaults=defs)\n    x = tf.stack(fields[:-1])\n    y = tf.stack(fields[-1:])\n    return (x - X_mean) / X_std, y","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:20.760158Z","iopub.execute_input":"2023-03-29T12:52:20.760591Z","iopub.status.idle":"2023-03-29T12:52:20.769073Z","shell.execute_reply.started":"2023-03-29T12:52:20.760552Z","shell.execute_reply":"2023-03-29T12:52:20.767566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:36.805067Z","iopub.execute_input":"2023-03-29T12:52:36.805510Z","iopub.status.idle":"2023-03-29T12:52:36.934374Z","shell.execute_reply.started":"2023-03-29T12:52:36.805456Z","shell.execute_reply":"2023-03-29T12:52:36.932697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n                       n_read_threads=None, shuffle_buffer_size=10000,\n                       n_parse_threads=5, batch_size=32):\n    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n    dataset = dataset.interleave(\n        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.batch(batch_size)\n    return dataset.prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:37.364428Z","iopub.execute_input":"2023-03-29T12:52:37.364960Z","iopub.status.idle":"2023-03-29T12:52:37.373144Z","shell.execute_reply.started":"2023-03-29T12:52:37.364913Z","shell.execute_reply":"2023-03-29T12:52:37.371986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\n\ntrain_set = csv_reader_dataset(train_filepaths, batch_size=3)\nfor X_batch, y_batch in train_set.take(2):\n    print(\"X =\", X_batch)\n    print(\"y =\", y_batch)\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:37.938843Z","iopub.execute_input":"2023-03-29T12:52:37.939316Z","iopub.status.idle":"2023-03-29T12:52:38.147937Z","shell.execute_reply.started":"2023-03-29T12:52:37.939270Z","shell.execute_reply":"2023-03-29T12:52:38.146555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = csv_reader_dataset(train_filepaths, repeat=None)\nvalid_set = csv_reader_dataset(valid_filepaths)\ntest_set = csv_reader_dataset(test_filepaths)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:39.054410Z","iopub.execute_input":"2023-03-29T12:52:39.054823Z","iopub.status.idle":"2023-03-29T12:52:39.178625Z","shell.execute_reply.started":"2023-03-29T12:52:39.054787Z","shell.execute_reply":"2023-03-29T12:52:39.177147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:40.822072Z","iopub.execute_input":"2023-03-29T12:52:40.822505Z","iopub.status.idle":"2023-03-29T12:52:41.133700Z","shell.execute_reply.started":"2023-03-29T12:52:40.822452Z","shell.execute_reply":"2023-03-29T12:52:41.132317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:41.171024Z","iopub.execute_input":"2023-03-29T12:52:41.171478Z","iopub.status.idle":"2023-03-29T12:52:41.191756Z","shell.execute_reply.started":"2023-03-29T12:52:41.171416Z","shell.execute_reply":"2023-03-29T12:52:41.190512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nmodel.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n          validation_data=valid_set)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:41.422568Z","iopub.execute_input":"2023-03-29T12:52:41.422998Z","iopub.status.idle":"2023-03-29T12:52:52.791606Z","shell.execute_reply.started":"2023-03-29T12:52:41.422959Z","shell.execute_reply":"2023-03-29T12:52:52.790421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_set, steps=len(X_test) // batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:52.793373Z","iopub.execute_input":"2023-03-29T12:52:52.793876Z","iopub.status.idle":"2023-03-29T12:52:53.159926Z","shell.execute_reply.started":"2023-03-29T12:52:52.793838Z","shell.execute_reply":"2023-03-29T12:52:53.158880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_set = test_set.map(lambda X, y: X) # we could instead just pass test_set, Keras would ignore the labels\nX_new = X_test\nmodel.predict(new_set, steps=len(X_new) // batch_size)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-29T12:52:55.688652Z","iopub.execute_input":"2023-03-29T12:52:55.689867Z","iopub.status.idle":"2023-03-29T12:52:56.207279Z","shell.execute_reply.started":"2023-03-29T12:52:55.689810Z","shell.execute_reply":"2023-03-29T12:52:56.205738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.mean_squared_error\n\nn_epochs = 5\nbatch_size = 32\nn_steps_per_epoch = len(X_train) // batch_size\ntotal_steps = n_epochs * n_steps_per_epoch\nglobal_step = 0\nfor X_batch, y_batch in train_set.take(total_steps):\n    global_step += 1\n    print(\"\\rGlobal step {}/{}\".format(global_step, total_steps), end=\"\")\n    with tf.GradientTape() as tape:\n        y_pred = model(X_batch)\n        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n        loss = tf.add_n([main_loss] + model.losses)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:52:57.487134Z","iopub.execute_input":"2023-03-29T12:52:57.488332Z","iopub.status.idle":"2023-03-29T12:53:38.475263Z","shell.execute_reply.started":"2023-03-29T12:52:57.488284Z","shell.execute_reply":"2023-03-29T12:53:38.473791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:53:38.478513Z","iopub.execute_input":"2023-03-29T12:53:38.479317Z","iopub.status.idle":"2023-03-29T12:53:38.503274Z","shell.execute_reply.started":"2023-03-29T12:53:38.479263Z","shell.execute_reply":"2023-03-29T12:53:38.501744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.mean_squared_error\n\n@tf.function\ndef train(model, n_epochs, batch_size=32,\n          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n    for X_batch, y_batch in train_set:\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\ntrain(model, 5)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:53:39.957233Z","iopub.execute_input":"2023-03-29T12:53:39.957672Z","iopub.status.idle":"2023-03-29T12:53:43.574504Z","shell.execute_reply.started":"2023-03-29T12:53:39.957633Z","shell.execute_reply":"2023-03-29T12:53:43.573143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:53:43.576941Z","iopub.execute_input":"2023-03-29T12:53:43.577342Z","iopub.status.idle":"2023-03-29T12:53:43.594176Z","shell.execute_reply.started":"2023-03-29T12:53:43.577303Z","shell.execute_reply":"2023-03-29T12:53:43.590848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = keras.optimizers.Nadam(learning_rate=0.01)\nloss_fn = keras.losses.mean_squared_error\n\n@tf.function\ndef train(model, n_epochs, batch_size=32,\n          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n    n_steps_per_epoch = len(X_train) // batch_size\n    total_steps = n_epochs * n_steps_per_epoch\n    global_step = 0\n    for X_batch, y_batch in train_set.take(total_steps):\n        global_step += 1\n        if tf.equal(global_step % 100, 0):\n            tf.print(\"\\rGlobal step\", global_step, \"/\", total_steps)\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\ntrain(model, 5)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:53:45.625420Z","iopub.execute_input":"2023-03-29T12:53:45.625901Z","iopub.status.idle":"2023-03-29T12:53:49.372070Z","shell.execute_reply.started":"2023-03-29T12:53:45.625860Z","shell.execute_reply":"2023-03-29T12:53:49.370562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is a short description of each method in the `Dataset` class:","metadata":{}},{"cell_type":"code","source":"for m in dir(tf.data.Dataset):\n    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n        func = getattr(tf.data.Dataset, m)\n        if hasattr(func, \"__doc__\"):\n            print(\"● {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-29T12:53:57.109815Z","iopub.execute_input":"2023-03-29T12:53:57.110226Z","iopub.status.idle":"2023-03-29T12:53:57.120151Z","shell.execute_reply.started":"2023-03-29T12:53:57.110190Z","shell.execute_reply":"2023-03-29T12:53:57.118700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The `TFRecord` binary format","metadata":{}},{"cell_type":"markdown","source":"A TFRecord file is just a list of binary records. You can create one using a `tf.io.TFRecordWriter`:","metadata":{}},{"cell_type":"code","source":"with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n    f.write(b\"This is the first record\")\n    f.write(b\"And this is the second record\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:00.549980Z","iopub.execute_input":"2023-03-29T12:54:00.550423Z","iopub.status.idle":"2023-03-29T12:54:00.557161Z","shell.execute_reply.started":"2023-03-29T12:54:00.550385Z","shell.execute_reply":"2023-03-29T12:54:00.555879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And you can read it using a `tf.data.TFRecordDataset`:","metadata":{}},{"cell_type":"code","source":"filepaths = [\"my_data.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filepaths)\nfor item in dataset:\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:01.481487Z","iopub.execute_input":"2023-03-29T12:54:01.481898Z","iopub.status.idle":"2023-03-29T12:54:01.522525Z","shell.execute_reply.started":"2023-03-29T12:54:01.481862Z","shell.execute_reply":"2023-03-29T12:54:01.521360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can read multiple TFRecord files with just one `TFRecordDataset`. By default it will read them one at a time, but if you set `num_parallel_reads=3`, it will read 3 at a time in parallel and interleave their records:","metadata":{}},{"cell_type":"code","source":"filepaths = [\"my_test_{}.tfrecord\".format(i) for i in range(5)]\nfor i, filepath in enumerate(filepaths):\n    with tf.io.TFRecordWriter(filepath) as f:\n        for j in range(3):\n            f.write(\"File {} record {}\".format(i, j).encode(\"utf-8\"))\n\ndataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)\nfor item in dataset:\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:03.177088Z","iopub.execute_input":"2023-03-29T12:54:03.177540Z","iopub.status.idle":"2023-03-29T12:54:03.227416Z","shell.execute_reply.started":"2023-03-29T12:54:03.177495Z","shell.execute_reply":"2023-03-29T12:54:03.226272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\nwith tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n    f.write(b\"This is the first record\")\n    f.write(b\"And this is the second record\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:05.474134Z","iopub.execute_input":"2023-03-29T12:54:05.474585Z","iopub.status.idle":"2023-03-29T12:54:05.484957Z","shell.execute_reply.started":"2023-03-29T12:54:05.474543Z","shell.execute_reply":"2023-03-29T12:54:05.482530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n                                  compression_type=\"GZIP\")\nfor item in dataset:\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:06.088628Z","iopub.execute_input":"2023-03-29T12:54:06.089105Z","iopub.status.idle":"2023-03-29T12:54:06.127592Z","shell.execute_reply.started":"2023-03-29T12:54:06.089066Z","shell.execute_reply":"2023-03-29T12:54:06.126120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A Brief Intro to Protocol Buffers","metadata":{}},{"cell_type":"markdown","source":"For this section you need to [install protobuf](https://developers.google.com/protocol-buffers/docs/downloads). In general you will not have to do so when using TensorFlow, as it comes with functions to create and parse protocol buffers of type `tf.train.Example`, which are generally sufficient. However, in this section we will learn about protocol buffers by creating our own simple protobuf definition, so we need the protobuf compiler (`protoc`): we will use it to compile the protobuf definition to a Python module that we can then use in our code.","metadata":{}},{"cell_type":"markdown","source":"First let's write a simple protobuf definition:","metadata":{}},{"cell_type":"code","source":"%%writefile person.proto\nsyntax = \"proto3\";\nmessage Person {\n  string name = 1;\n  int32 id = 2;\n  repeated string email = 3;\n}","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:08.166564Z","iopub.execute_input":"2023-03-29T12:54:08.167569Z","iopub.status.idle":"2023-03-29T12:54:08.178117Z","shell.execute_reply.started":"2023-03-29T12:54:08.167491Z","shell.execute_reply":"2023-03-29T12:54:08.176585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And let's compile it (the `--descriptor_set_out` and `--include_imports` options are only required for the `tf.io.decode_proto()` example below):","metadata":{}},{"cell_type":"code","source":"!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:09.090247Z","iopub.execute_input":"2023-03-29T12:54:09.090732Z","iopub.status.idle":"2023-03-29T12:54:10.253116Z","shell.execute_reply.started":"2023-03-29T12:54:09.090686Z","shell.execute_reply":"2023-03-29T12:54:10.250727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls person*","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:10.256252Z","iopub.execute_input":"2023-03-29T12:54:10.257307Z","iopub.status.idle":"2023-03-29T12:54:11.386031Z","shell.execute_reply.started":"2023-03-29T12:54:10.257253Z","shell.execute_reply":"2023-03-29T12:54:11.384297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from person_pb2 import Person\n\nperson = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\nprint(person)  # display the Person","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:12.521120Z","iopub.execute_input":"2023-03-29T12:54:12.521616Z","iopub.status.idle":"2023-03-29T12:54:12.532691Z","shell.execute_reply.started":"2023-03-29T12:54:12.521570Z","shell.execute_reply":"2023-03-29T12:54:12.531177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"person.name  # read a field","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:13.025818Z","iopub.execute_input":"2023-03-29T12:54:13.026281Z","iopub.status.idle":"2023-03-29T12:54:13.035035Z","shell.execute_reply.started":"2023-03-29T12:54:13.026238Z","shell.execute_reply":"2023-03-29T12:54:13.033650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"person.name = \"Alice\"  # modify a field","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:15.042765Z","iopub.execute_input":"2023-03-29T12:54:15.043219Z","iopub.status.idle":"2023-03-29T12:54:15.048409Z","shell.execute_reply.started":"2023-03-29T12:54:15.043177Z","shell.execute_reply":"2023-03-29T12:54:15.047352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"person.email[0]  # repeated fields can be accessed like arrays","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:15.359824Z","iopub.execute_input":"2023-03-29T12:54:15.360521Z","iopub.status.idle":"2023-03-29T12:54:15.367512Z","shell.execute_reply.started":"2023-03-29T12:54:15.360457Z","shell.execute_reply":"2023-03-29T12:54:15.366137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"person.email.append(\"c@d.com\")  # add an email address","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:15.625382Z","iopub.execute_input":"2023-03-29T12:54:15.626809Z","iopub.status.idle":"2023-03-29T12:54:15.633488Z","shell.execute_reply.started":"2023-03-29T12:54:15.626721Z","shell.execute_reply":"2023-03-29T12:54:15.632144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = person.SerializeToString()  # serialize to a byte string\ns","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:18.663668Z","iopub.execute_input":"2023-03-29T12:54:18.664112Z","iopub.status.idle":"2023-03-29T12:54:18.672865Z","shell.execute_reply.started":"2023-03-29T12:54:18.664073Z","shell.execute_reply":"2023-03-29T12:54:18.671034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"person2 = Person()  # create a new Person\nperson2.ParseFromString(s)  # parse the byte string (27 bytes)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:19.416737Z","iopub.execute_input":"2023-03-29T12:54:19.417180Z","iopub.status.idle":"2023-03-29T12:54:19.425079Z","shell.execute_reply.started":"2023-03-29T12:54:19.417138Z","shell.execute_reply":"2023-03-29T12:54:19.423564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"person == person2  # now they are equal","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:20.005122Z","iopub.execute_input":"2023-03-29T12:54:20.005690Z","iopub.status.idle":"2023-03-29T12:54:20.014729Z","shell.execute_reply.started":"2023-03-29T12:54:20.005638Z","shell.execute_reply":"2023-03-29T12:54:20.013226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Custom protobuf","metadata":{}},{"cell_type":"markdown","source":"In rare cases, you may want to parse a custom protobuf (like the one we just created) in TensorFlow. For this you can use the `tf.io.decode_proto()` function:","metadata":{}},{"cell_type":"code","source":"person_tf = tf.io.decode_proto(\n    bytes=s,\n    message_type=\"Person\",\n    field_names=[\"name\", \"id\", \"email\"],\n    output_types=[tf.string, tf.int32, tf.string],\n    descriptor_source=\"person.desc\")\n\nperson_tf.values","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:22.265980Z","iopub.execute_input":"2023-03-29T12:54:22.267110Z","iopub.status.idle":"2023-03-29T12:54:22.286297Z","shell.execute_reply.started":"2023-03-29T12:54:22.267030Z","shell.execute_reply":"2023-03-29T12:54:22.284251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For more details, see the [`tf.io.decode_proto()`](https://www.tensorflow.org/api_docs/python/tf/io/decode_proto) documentation.","metadata":{}},{"cell_type":"markdown","source":"### TensorFlow Protobufs","metadata":{}},{"cell_type":"markdown","source":"Here is the definition of the tf.train.Example protobuf:","metadata":{}},{"cell_type":"markdown","source":"```proto\nsyntax = \"proto3\";\n\nmessage BytesList { repeated bytes value = 1; }\nmessage FloatList { repeated float value = 1 [packed = true]; }\nmessage Int64List { repeated int64 value = 1 [packed = true]; }\nmessage Feature {\n    oneof kind {\n        BytesList bytes_list = 1;\n        FloatList float_list = 2;\n        Int64List int64_list = 3;\n    }\n};\nmessage Features { map<string, Feature> feature = 1; };\nmessage Example { Features features = 1; };\n```","metadata":{}},{"cell_type":"markdown","source":"**Warning**: in TensorFlow 2.0 and 2.1, there was a bug preventing `from tensorflow.train import X` so we work around it by writing `X = tf.train.X`. See https://github.com/tensorflow/tensorflow/issues/33289 for more details.","metadata":{}},{"cell_type":"code","source":"#from tensorflow.train import BytesList, FloatList, Int64List\n#from tensorflow.train import Feature, Features, Example\nBytesList = tf.train.BytesList\nFloatList = tf.train.FloatList\nInt64List = tf.train.Int64List\nFeature = tf.train.Feature\nFeatures = tf.train.Features\nExample = tf.train.Example\n\nperson_example = Example(\n    features=Features(\n        feature={\n            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n            \"id\": Feature(int64_list=Int64List(value=[123])),\n            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n        }))\n\nwith tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n    f.write(person_example.SerializeToString())","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:20:24.321031Z","iopub.execute_input":"2023-03-29T17:20:24.321502Z","iopub.status.idle":"2023-03-29T17:20:24.337630Z","shell.execute_reply.started":"2023-03-29T17:20:24.321451Z","shell.execute_reply":"2023-03-29T17:20:24.335987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_description = {\n    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    \"emails\": tf.io.VarLenFeature(tf.string),\n}\nfor serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n    parsed_example = tf.io.parse_single_example(serialized_example,\n                                                feature_description)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:27.012295Z","iopub.execute_input":"2023-03-29T12:54:27.013227Z","iopub.status.idle":"2023-03-29T12:54:27.060984Z","shell.execute_reply.started":"2023-03-29T12:54:27.013170Z","shell.execute_reply":"2023-03-29T12:54:27.059378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_example","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:27.334019Z","iopub.execute_input":"2023-03-29T12:54:27.334665Z","iopub.status.idle":"2023-03-29T12:54:27.346129Z","shell.execute_reply.started":"2023-03-29T12:54:27.334611Z","shell.execute_reply":"2023-03-29T12:54:27.344444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_example","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-29T12:54:27.546352Z","iopub.execute_input":"2023-03-29T12:54:27.547481Z","iopub.status.idle":"2023-03-29T12:54:27.555612Z","shell.execute_reply.started":"2023-03-29T12:54:27.547396Z","shell.execute_reply":"2023-03-29T12:54:27.553937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_example[\"emails\"].values[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:28.326215Z","iopub.execute_input":"2023-03-29T12:54:28.326725Z","iopub.status.idle":"2023-03-29T12:54:28.340124Z","shell.execute_reply.started":"2023-03-29T12:54:28.326674Z","shell.execute_reply":"2023-03-29T12:54:28.338616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:29.631153Z","iopub.execute_input":"2023-03-29T12:54:29.631623Z","iopub.status.idle":"2023-03-29T12:54:29.646347Z","shell.execute_reply.started":"2023-03-29T12:54:29.631583Z","shell.execute_reply":"2023-03-29T12:54:29.644555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_example[\"emails\"].values","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:30.382437Z","iopub.execute_input":"2023-03-29T12:54:30.383927Z","iopub.status.idle":"2023-03-29T12:54:30.393712Z","shell.execute_reply.started":"2023-03-29T12:54:30.383851Z","shell.execute_reply":"2023-03-29T12:54:30.392217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Putting Images in TFRecords","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_sample_images\n\nimg = load_sample_images()[\"images\"][0]\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Original Image\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:30.992192Z","iopub.execute_input":"2023-03-29T12:54:30.992689Z","iopub.status.idle":"2023-03-29T12:54:31.333680Z","shell.execute_reply.started":"2023-03-29T12:54:30.992644Z","shell.execute_reply":"2023-03-29T12:54:31.332221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = tf.io.encode_jpeg(img)\nexample_with_image = Example(features=Features(feature={\n    \"image\": Feature(bytes_list=BytesList(value=[data.numpy()]))}))\nserialized_example = example_with_image.SerializeToString()\n# then save to TFRecord","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:32.350002Z","iopub.execute_input":"2023-03-29T12:54:32.350689Z","iopub.status.idle":"2023-03-29T12:54:32.368990Z","shell.execute_reply.started":"2023-03-29T12:54:32.350645Z","shell.execute_reply":"2023-03-29T12:54:32.367156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_description = { \"image\": tf.io.VarLenFeature(tf.string) }\nexample_with_image = tf.io.parse_single_example(serialized_example, feature_description)\ndecoded_img = tf.io.decode_jpeg(example_with_image[\"image\"].values[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:32.683795Z","iopub.execute_input":"2023-03-29T12:54:32.685091Z","iopub.status.idle":"2023-03-29T12:54:32.698873Z","shell.execute_reply.started":"2023-03-29T12:54:32.685033Z","shell.execute_reply":"2023-03-29T12:54:32.697716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or use `decode_image()` which supports BMP, GIF, JPEG and PNG formats:","metadata":{}},{"cell_type":"code","source":"decoded_img = tf.io.decode_image(example_with_image[\"image\"].values[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:33.239425Z","iopub.execute_input":"2023-03-29T12:54:33.239975Z","iopub.status.idle":"2023-03-29T12:54:33.251698Z","shell.execute_reply.started":"2023-03-29T12:54:33.239923Z","shell.execute_reply":"2023-03-29T12:54:33.250070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(decoded_img)\nplt.title(\"Decoded Image\")\n#plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:34.657299Z","iopub.execute_input":"2023-03-29T12:54:34.657875Z","iopub.status.idle":"2023-03-29T12:54:35.030156Z","shell.execute_reply.started":"2023-03-29T12:54:34.657824Z","shell.execute_reply":"2023-03-29T12:54:35.028540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Putting Tensors and Sparse Tensors in TFRecords","metadata":{}},{"cell_type":"markdown","source":"Tensors can be serialized and parsed easily using `tf.io.serialize_tensor()` and `tf.io.parse_tensor()`:","metadata":{}},{"cell_type":"code","source":"t = tf.constant([[0., 1.], [2., 3.], [4., 5.]])\ns = tf.io.serialize_tensor(t)\ns","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:50.732691Z","iopub.execute_input":"2023-03-29T12:54:50.733126Z","iopub.status.idle":"2023-03-29T12:54:50.744745Z","shell.execute_reply.started":"2023-03-29T12:54:50.733086Z","shell.execute_reply":"2023-03-29T12:54:50.743134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.io.parse_tensor(s, out_type=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:51.307907Z","iopub.execute_input":"2023-03-29T12:54:51.308321Z","iopub.status.idle":"2023-03-29T12:54:51.318745Z","shell.execute_reply.started":"2023-03-29T12:54:51.308285Z","shell.execute_reply":"2023-03-29T12:54:51.317367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"serialized_sparse = tf.io.serialize_sparse(parsed_example[\"emails\"])\nserialized_sparse","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:51.718272Z","iopub.execute_input":"2023-03-29T12:54:51.719098Z","iopub.status.idle":"2023-03-29T12:54:51.731286Z","shell.execute_reply.started":"2023-03-29T12:54:51.719048Z","shell.execute_reply":"2023-03-29T12:54:51.729932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BytesList(value=serialized_sparse.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:53.736976Z","iopub.execute_input":"2023-03-29T12:54:53.737418Z","iopub.status.idle":"2023-03-29T12:54:53.745656Z","shell.execute_reply.started":"2023-03-29T12:54:53.737378Z","shell.execute_reply":"2023-03-29T12:54:53.744406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\nfor serialized_examples in dataset:\n    parsed_examples = tf.io.parse_example(serialized_examples,\n                                          feature_description)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:54.990740Z","iopub.execute_input":"2023-03-29T12:54:54.991139Z","iopub.status.idle":"2023-03-29T12:54:55.030890Z","shell.execute_reply.started":"2023-03-29T12:54:54.991105Z","shell.execute_reply":"2023-03-29T12:54:55.029572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_examples","metadata":{"execution":{"iopub.status.busy":"2023-03-29T12:54:56.449310Z","iopub.execute_input":"2023-03-29T12:54:56.449803Z","iopub.status.idle":"2023-03-29T12:54:56.459137Z","shell.execute_reply.started":"2023-03-29T12:54:56.449757Z","shell.execute_reply":"2023-03-29T12:54:56.457533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Sequential Data Using `SequenceExample`","metadata":{}},{"cell_type":"markdown","source":"```proto\nsyntax = \"proto3\";\n\nmessage FeatureList { repeated Feature feature = 1; };\nmessage FeatureLists { map<string, FeatureList> feature_list = 1; };\nmessage SequenceExample {\n  Features context = 1;\n  FeatureLists feature_lists = 2;\n};\n```","metadata":{}},{"cell_type":"markdown","source":"**Warning**: in TensorFlow 2.0 and 2.1, there was a bug preventing `from tensorflow.train import X` so we work around it by writing `X = tf.train.X`. See https://github.com/tensorflow/tensorflow/issues/33289 for more details.","metadata":{}},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:21:54.326843Z","iopub.execute_input":"2023-03-29T13:21:54.327231Z","iopub.status.idle":"2023-03-29T13:21:54.353348Z","shell.execute_reply.started":"2023-03-29T13:21:54.327198Z","shell.execute_reply":"2023-03-29T13:21:54.351686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from tensorflow.train import FeatureList, FeatureLists, SequenceExample\nFeatureList = tf.train.FeatureList\nFeatureLists = tf.train.FeatureLists\nSequenceExample = tf.train.SequenceExample\n\ncontext = Features(feature={\n    \"author_id\": Feature(int64_list=Int64List(value=[123])),\n    \"title\": Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])),\n    \"pub_date\": Feature(int64_list=Int64List(value=[1623, 12, 25]))\n})\n\ncontent = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\ncomments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n\ndef words_to_feature(words):\n    return Feature(bytes_list=BytesList(value=[word.encode(\"utf-8\")\n                                               for word in words]))\n\ncontent_features = [words_to_feature(sentence) for sentence in content]\ncomments_features = [words_to_feature(comment) for comment in comments]\n            \nsequence_example = SequenceExample(\n    context=context,\n    feature_lists=FeatureLists(feature_list={\n        \"content\": FeatureList(feature=content_features),\n        \"comments\": FeatureList(feature=comments_features)\n    }))","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:23:36.056115Z","iopub.execute_input":"2023-03-29T13:23:36.056570Z","iopub.status.idle":"2023-03-29T13:23:36.068763Z","shell.execute_reply.started":"2023-03-29T13:23:36.056530Z","shell.execute_reply":"2023-03-29T13:23:36.067641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_example","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:57:01.879844Z","iopub.execute_input":"2023-03-29T13:57:01.880527Z","iopub.status.idle":"2023-03-29T13:57:01.898827Z","shell.execute_reply.started":"2023-03-29T13:57:01.880464Z","shell.execute_reply":"2023-03-29T13:57:01.895670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"serialized_sequence_example = sequence_example.SerializeToString()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:57:09.035071Z","iopub.execute_input":"2023-03-29T13:57:09.035548Z","iopub.status.idle":"2023-03-29T13:57:09.042789Z","shell.execute_reply.started":"2023-03-29T13:57:09.035509Z","shell.execute_reply":"2023-03-29T13:57:09.041129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_feature_descriptions = {\n    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    \"title\": tf.io.VarLenFeature(tf.string),\n    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n}\nsequence_feature_descriptions = {\n    \"content\": tf.io.VarLenFeature(tf.string),\n    \"comments\": tf.io.VarLenFeature(tf.string),\n}\nparsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n    serialized_sequence_example, context_feature_descriptions,\n    sequence_feature_descriptions)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:57:10.400958Z","iopub.execute_input":"2023-03-29T13:57:10.401925Z","iopub.status.idle":"2023-03-29T13:57:10.450362Z","shell.execute_reply.started":"2023-03-29T13:57:10.401862Z","shell.execute_reply":"2023-03-29T13:57:10.449079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_context","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:57:12.021212Z","iopub.execute_input":"2023-03-29T13:57:12.021639Z","iopub.status.idle":"2023-03-29T13:57:12.032433Z","shell.execute_reply.started":"2023-03-29T13:57:12.021605Z","shell.execute_reply":"2023-03-29T13:57:12.031253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_context[\"title\"].values","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:57:13.068935Z","iopub.execute_input":"2023-03-29T13:57:13.069806Z","iopub.status.idle":"2023-03-29T13:57:13.078431Z","shell.execute_reply.started":"2023-03-29T13:57:13.069764Z","shell.execute_reply":"2023-03-29T13:57:13.076919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_feature_lists","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:57:13.995276Z","iopub.execute_input":"2023-03-29T13:57:13.996645Z","iopub.status.idle":"2023-03-29T13:57:14.007042Z","shell.execute_reply.started":"2023-03-29T13:57:13.996575Z","shell.execute_reply":"2023-03-29T13:57:14.005527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"]))","metadata":{"execution":{"iopub.status.busy":"2023-03-29T13:57:17.000324Z","iopub.execute_input":"2023-03-29T13:57:17.000782Z","iopub.status.idle":"2023-03-29T13:57:17.163380Z","shell.execute_reply.started":"2023-03-29T13:57:17.000719Z","shell.execute_reply":"2023-03-29T13:57:17.161675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Features API","metadata":{}},{"cell_type":"markdown","source":"Let's use the variant of the California housing dataset that we used in Chapter 2, since it contains categorical features and missing values:","metadata":{}},{"cell_type":"code","source":"import os\nimport tarfile\nimport urllib.request\n\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    os.makedirs(housing_path, exist_ok=True)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:11.297689Z","iopub.execute_input":"2023-03-29T14:13:11.298203Z","iopub.status.idle":"2023-03-29T14:13:11.306896Z","shell.execute_reply.started":"2023-03-29T14:13:11.298163Z","shell.execute_reply":"2023-03-29T14:13:11.305492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fetch_housing_data()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:13.011917Z","iopub.execute_input":"2023-03-29T14:13:13.012340Z","iopub.status.idle":"2023-03-29T14:13:13.647591Z","shell.execute_reply.started":"2023-03-29T14:13:13.012300Z","shell.execute_reply":"2023-03-29T14:13:13.646628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:13.649199Z","iopub.execute_input":"2023-03-29T14:13:13.649516Z","iopub.status.idle":"2023-03-29T14:13:13.655193Z","shell.execute_reply.started":"2023-03-29T14:13:13.649486Z","shell.execute_reply":"2023-03-29T14:13:13.653606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing = load_housing_data()\nhousing.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:14.998666Z","iopub.execute_input":"2023-03-29T14:13:14.999618Z","iopub.status.idle":"2023-03-29T14:13:15.065083Z","shell.execute_reply.started":"2023-03-29T14:13:14.999574Z","shell.execute_reply":"2023-03-29T14:13:15.063819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:16.031958Z","iopub.execute_input":"2023-03-29T14:13:16.032585Z","iopub.status.idle":"2023-03-29T14:13:16.037970Z","shell.execute_reply.started":"2023-03-29T14:13:16.032537Z","shell.execute_reply":"2023-03-29T14:13:16.037023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_mean, age_std = X_mean[1], X_std[1]  # The median age is column in 1\nhousing_median_age = tf.feature_column.numeric_column(\n    \"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:16.704794Z","iopub.execute_input":"2023-03-29T14:13:16.705236Z","iopub.status.idle":"2023-03-29T14:13:16.733808Z","shell.execute_reply.started":"2023-03-29T14:13:16.705174Z","shell.execute_reply":"2023-03-29T14:13:16.732306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"median_income = tf.feature_column.numeric_column(\"median_income\")\nbucketized_income = tf.feature_column.bucketized_column(\n    median_income, boundaries=[1.5, 3., 4.5, 6.])","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:17.271237Z","iopub.execute_input":"2023-03-29T14:13:17.271653Z","iopub.status.idle":"2023-03-29T14:13:17.277876Z","shell.execute_reply.started":"2023-03-29T14:13:17.271618Z","shell.execute_reply":"2023-03-29T14:13:17.276823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bucketized_income","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:23.738912Z","iopub.execute_input":"2023-03-29T14:13:23.739316Z","iopub.status.idle":"2023-03-29T14:13:23.747103Z","shell.execute_reply.started":"2023-03-29T14:13:23.739282Z","shell.execute_reply":"2023-03-29T14:13:23.745755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\nocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\n    \"ocean_proximity\", ocean_prox_vocab)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:25.089383Z","iopub.execute_input":"2023-03-29T14:13:25.089812Z","iopub.status.idle":"2023-03-29T14:13:25.096032Z","shell.execute_reply.started":"2023-03-29T14:13:25.089771Z","shell.execute_reply":"2023-03-29T14:13:25.094723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ocean_proximity","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:26.301142Z","iopub.execute_input":"2023-03-29T14:13:26.301926Z","iopub.status.idle":"2023-03-29T14:13:26.308696Z","shell.execute_reply.started":"2023-03-29T14:13:26.301881Z","shell.execute_reply":"2023-03-29T14:13:26.307614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Just an example, it's not used later on\ncity_hash = tf.feature_column.categorical_column_with_hash_bucket(\n    \"city\", hash_bucket_size=1000)\ncity_hash","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:27.613655Z","iopub.execute_input":"2023-03-29T14:13:27.614072Z","iopub.status.idle":"2023-03-29T14:13:27.622499Z","shell.execute_reply.started":"2023-03-29T14:13:27.614035Z","shell.execute_reply":"2023-03-29T14:13:27.621220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bucketized_age = tf.feature_column.bucketized_column(\n    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled\nage_and_ocean_proximity = tf.feature_column.crossed_column(\n    [bucketized_age, ocean_proximity], hash_bucket_size=100)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:29.399781Z","iopub.execute_input":"2023-03-29T14:13:29.400188Z","iopub.status.idle":"2023-03-29T14:13:29.406293Z","shell.execute_reply.started":"2023-03-29T14:13:29.400152Z","shell.execute_reply":"2023-03-29T14:13:29.405356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latitude = tf.feature_column.numeric_column(\"latitude\")\nlongitude = tf.feature_column.numeric_column(\"longitude\")\nbucketized_latitude = tf.feature_column.bucketized_column(\n    latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\nbucketized_longitude = tf.feature_column.bucketized_column(\n    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))\nlocation = tf.feature_column.crossed_column(\n    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:43.613038Z","iopub.execute_input":"2023-03-29T14:13:43.613444Z","iopub.status.idle":"2023-03-29T14:13:43.621615Z","shell.execute_reply.started":"2023-03-29T14:13:43.613408Z","shell.execute_reply":"2023-03-29T14:13:43.620251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:44.643695Z","iopub.execute_input":"2023-03-29T14:13:44.644116Z","iopub.status.idle":"2023-03-29T14:13:44.649211Z","shell.execute_reply.started":"2023-03-29T14:13:44.644079Z","shell.execute_reply":"2023-03-29T14:13:44.648055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\n                                                           dimension=2)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:46.857679Z","iopub.execute_input":"2023-03-29T14:13:46.858540Z","iopub.status.idle":"2023-03-29T14:13:46.863926Z","shell.execute_reply.started":"2023-03-29T14:13:46.858499Z","shell.execute_reply":"2023-03-29T14:13:46.862692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Feature Columns for Parsing","metadata":{}},{"cell_type":"code","source":"median_house_value = tf.feature_column.numeric_column(\"median_house_value\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:48.749287Z","iopub.execute_input":"2023-03-29T14:13:48.749673Z","iopub.status.idle":"2023-03-29T14:13:48.755559Z","shell.execute_reply.started":"2023-03-29T14:13:48.749639Z","shell.execute_reply":"2023-03-29T14:13:48.754391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [housing_median_age, median_house_value]\nfeature_descriptions = tf.feature_column.make_parse_example_spec(columns)\nfeature_descriptions","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:50.091291Z","iopub.execute_input":"2023-03-29T14:13:50.092126Z","iopub.status.idle":"2023-03-29T14:13:50.100480Z","shell.execute_reply.started":"2023-03-29T14:13:50.092080Z","shell.execute_reply":"2023-03-29T14:13:50.099236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.io.TFRecordWriter(\"my_data_with_features.tfrecords\") as f:\n    for x, y in zip(X_train[:, 1:2], y_train):\n        example = Example(features=Features(feature={\n            \"housing_median_age\": Feature(float_list=FloatList(value=[x])),\n            \"median_house_value\": Feature(float_list=FloatList(value=[y]))\n        }))\n        f.write(example.SerializeToString())","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:52.537163Z","iopub.execute_input":"2023-03-29T14:13:52.537540Z","iopub.status.idle":"2023-03-29T14:13:52.566407Z","shell.execute_reply.started":"2023-03-29T14:13:52.537505Z","shell.execute_reply":"2023-03-29T14:13:52.564686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:55.295722Z","iopub.execute_input":"2023-03-29T14:13:55.296113Z","iopub.status.idle":"2023-03-29T14:13:55.311452Z","shell.execute_reply.started":"2023-03-29T14:13:55.296080Z","shell.execute_reply":"2023-03-29T14:13:55.310151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_examples(serialized_examples):\n    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n    targets = examples.pop(\"median_house_value\") # separate the targets\n    return examples, targets\n\nbatch_size = 32\ndataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\ndataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:56.023772Z","iopub.execute_input":"2023-03-29T14:13:56.024150Z","iopub.status.idle":"2023-03-29T14:13:56.137483Z","shell.execute_reply.started":"2023-03-29T14:13:56.024117Z","shell.execute_reply":"2023-03-29T14:13:56.136031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Warning**: the `DenseFeatures` layer currently does not work with the Functional API, see [TF issue #27416](https://github.com/tensorflow/tensorflow/issues/27416). Hopefully this will be resolved before the final release of TF 2.0.","metadata":{}},{"cell_type":"code","source":"columns_without_target = columns[:-1]\nmodel = keras.models.Sequential([\n    keras.layers.DenseFeatures(feature_columns=columns_without_target),\n    keras.layers.Dense(1)\n])\nmodel.compile(loss=\"mse\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\nmodel.fit(dataset, steps_per_epoch=len(X_train) // batch_size, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:13:58.413125Z","iopub.execute_input":"2023-03-29T14:13:58.413788Z","iopub.status.idle":"2023-03-29T14:13:58.479844Z","shell.execute_reply.started":"2023-03-29T14:13:58.413716Z","shell.execute_reply":"2023-03-29T14:13:58.478204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"some_columns = [ocean_proximity_embed, bucketized_income]\ndense_features = keras.layers.DenseFeatures(some_columns)\ndense_features({\n    \"ocean_proximity\": [[\"NEAR OCEAN\"], [\"INLAND\"], [\"INLAND\"]],\n    \"median_income\": [[3.], [7.2], [1.]]\n})","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:00.345044Z","iopub.execute_input":"2023-03-29T14:14:00.345727Z","iopub.status.idle":"2023-03-29T14:14:00.740114Z","shell.execute_reply.started":"2023-03-29T14:14:00.345677Z","shell.execute_reply":"2023-03-29T14:14:00.738822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF Transform","metadata":{}},{"cell_type":"code","source":"try:\n    import tensorflow_transform as tft\n\n    def preprocess(inputs):  # inputs is a batch of input features\n        median_age = inputs[\"housing_median_age\"]\n        ocean_proximity = inputs[\"ocean_proximity\"]\n        standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n        ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n        return {\n            \"standardized_median_age\": standardized_age,\n            \"ocean_proximity_id\": ocean_proximity_id\n        }\nexcept ImportError:\n    print(\"TF Transform is not installed. Try running: pip3 install -U tensorflow-transform\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:02.699906Z","iopub.execute_input":"2023-03-29T14:14:02.700609Z","iopub.status.idle":"2023-03-29T14:14:03.127405Z","shell.execute_reply.started":"2023-03-29T14:14:02.700557Z","shell.execute_reply":"2023-03-29T14:14:03.126532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TensorFlow Datasets","metadata":{}},{"cell_type":"code","source":"import tensorflow_datasets as tfds\n\ndatasets = tfds.load(name=\"mnist\")\nmnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:06.978668Z","iopub.execute_input":"2023-03-29T14:14:06.979097Z","iopub.status.idle":"2023-03-29T14:14:11.843774Z","shell.execute_reply.started":"2023-03-29T14:14:06.979060Z","shell.execute_reply":"2023-03-29T14:14:11.842510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tfds.list_builders())","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:14.276451Z","iopub.execute_input":"2023-03-29T14:14:14.276864Z","iopub.status.idle":"2023-03-29T14:14:15.968361Z","shell.execute_reply.started":"2023-03-29T14:14:14.276826Z","shell.execute_reply":"2023-03-29T14:14:15.967147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,3))\nmnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\nfor item in mnist_train:\n    images = item[\"image\"]\n    labels = item[\"label\"]\n    for index in range(5):\n        plt.subplot(1, 5, index + 1)\n        image = images[index, ..., 0]\n        label = labels[index].numpy()\n        plt.imshow(image, cmap=\"binary\")\n        plt.title(label)\n        plt.axis(\"off\")\n    break # just showing part of the first batch","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:16.850961Z","iopub.execute_input":"2023-03-29T14:14:16.852189Z","iopub.status.idle":"2023-03-29T14:14:17.243188Z","shell.execute_reply.started":"2023-03-29T14:14:16.852129Z","shell.execute_reply":"2023-03-29T14:14:17.241371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = tfds.load(name=\"mnist\")\nmnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\nmnist_train = mnist_train.repeat(5).batch(32)\nmnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\nmnist_train = mnist_train.prefetch(1)\nfor images, labels in mnist_train.take(1):\n    print(images.shape)\n    print(labels.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:21.161898Z","iopub.execute_input":"2023-03-29T14:14:21.162322Z","iopub.status.idle":"2023-03-29T14:14:21.342600Z","shell.execute_reply.started":"2023-03-29T14:14:21.162277Z","shell.execute_reply":"2023-03-29T14:14:21.341444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:22.924493Z","iopub.execute_input":"2023-03-29T14:14:22.924935Z","iopub.status.idle":"2023-03-29T14:14:22.944971Z","shell.execute_reply.started":"2023-03-29T14:14:22.924897Z","shell.execute_reply":"2023-03-29T14:14:22.943797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\nmnist_train = datasets[\"train\"].repeat().prefetch(1)\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28, 1]),\n    keras.layers.Lambda(lambda images: tf.cast(images, tf.float32)),\n    keras.layers.Dense(10, activation=\"softmax\")])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\nmodel.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:30.890268Z","iopub.execute_input":"2023-03-29T14:14:30.890678Z","iopub.status.idle":"2023-03-29T14:14:51.949135Z","shell.execute_reply.started":"2023-03-29T14:14:30.890642Z","shell.execute_reply":"2023-03-29T14:14:51.947780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TensorFlow Hub","metadata":{}},{"cell_type":"code","source":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:51.951073Z","iopub.execute_input":"2023-03-29T14:14:51.951433Z","iopub.status.idle":"2023-03-29T14:14:51.965297Z","shell.execute_reply.started":"2023-03-29T14:14:51.951399Z","shell.execute_reply":"2023-03-29T14:14:51.963950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub\n\nhub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\",\n                           output_shape=[50], input_shape=[], dtype=tf.string)\n\nmodel = keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(keras.layers.Dense(16, activation='relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:14:51.966782Z","iopub.execute_input":"2023-03-29T14:14:51.967152Z","iopub.status.idle":"2023-03-29T14:15:01.351231Z","shell.execute_reply.started":"2023-03-29T14:14:51.967115Z","shell.execute_reply":"2023-03-29T14:15:01.349952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = tf.constant([\"It was a great movie\", \"The actors were amazing\"])\nembeddings = hub_layer(sentences)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T14:15:01.354020Z","iopub.execute_input":"2023-03-29T14:15:01.354793Z","iopub.status.idle":"2023-03-29T14:15:01.384920Z","shell.execute_reply.started":"2023-03-29T14:15:01.354752Z","shell.execute_reply":"2023-03-29T14:15:01.383325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Own solutions to exercises","metadata":{}},{"cell_type":"markdown","source":"## Exercise 9","metadata":{}},{"cell_type":"code","source":"fashion_mnist = keras.datasets.fashion_mnist\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\nX_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_test = X_test / 255.","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:20:44.221013Z","iopub.execute_input":"2023-03-29T17:20:44.221920Z","iopub.status.idle":"2023-03-29T17:20:45.329286Z","shell.execute_reply.started":"2023-03-29T17:20:44.221866Z","shell.execute_reply":"2023-03-29T17:20:45.328015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size = len(X_train))\nvalid_data = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\ntest_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:45:52.216805Z","iopub.execute_input":"2023-03-29T17:45:52.218078Z","iopub.status.idle":"2023-03-29T17:45:52.681639Z","shell.execute_reply.started":"2023-03-29T17:45:52.218012Z","shell.execute_reply":"2023-03-29T17:45:52.680234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.take(1)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:46:08.296129Z","iopub.execute_input":"2023-03-29T17:46:08.296524Z","iopub.status.idle":"2023-03-29T17:46:08.307503Z","shell.execute_reply.started":"2023-03-29T17:46:08.296490Z","shell.execute_reply":"2023-03-29T17:46:08.306216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef save_tfrec_files(data, prefix, numParts = 10):\n    dataDir = os.path.join(\"datasets\", \"mnist\")\n    os.makedirs(dataDir, exist_ok = True)\n    pathFormat = os.path.join(dataDir, prefix + \"_{:02d}.tfr\")\n    \n    filePaths = []\n    dataLength = len(data)\n    partLength = math.ceil(dataLength / numParts)\n    f = None\n    fileIndex = 0\n    partIndex = 0\n    for (image, value) in data:\n        if f is None:\n            path = pathFormat.format(fileIndex)\n            filePaths.append(path)\n            f = tf.io.TFRecordWriter(path)\n            fileIndex += 1\n            partIndex = 0\n        example = Example(\n            features = Features(\n                feature = {\n                    \"image\": Feature(bytes_list = BytesList(value=[tf.io.serialize_tensor(image).numpy()])),\n                     \"value\": Feature(int64_list = Int64List(value=[value]))\n                        }\n                )\n            )\n        f.write(example.SerializeToString())\n        partIndex += 1\n        if partIndex>=partLength:\n            f.close()\n            f = None\n            \n    if f is not None:\n        f.close()\n        \n    return filePaths\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:23:37.986121Z","iopub.execute_input":"2023-03-29T17:23:37.987551Z","iopub.status.idle":"2023-03-29T17:23:37.999273Z","shell.execute_reply.started":"2023-03-29T17:23:37.987494Z","shell.execute_reply":"2023-03-29T17:23:37.997806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_paths = save_tfrec_files(train_data, \"train\")\nvalid_paths = save_tfrec_files(valid_data, \"valid\")\ntest_paths = save_tfrec_files(test_data, \"test\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:23:39.644901Z","iopub.execute_input":"2023-03-29T17:23:39.646150Z","iopub.status.idle":"2023-03-29T17:24:10.988469Z","shell.execute_reply.started":"2023-03-29T17:23:39.646087Z","shell.execute_reply":"2023-03-29T17:24:10.987276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_paths, valid_paths, test_paths","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:24:10.990462Z","iopub.execute_input":"2023-03-29T17:24:10.990847Z","iopub.status.idle":"2023-03-29T17:24:10.999069Z","shell.execute_reply.started":"2023-03-29T17:24:10.990806Z","shell.execute_reply":"2023-03-29T17:24:10.997803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnistFeatureDescriptions = {\n    \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n    \"value\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n}\n\n@tf.function\ndef decode_example(serializedExample):\n    example = tf.io.parse_example(serializedExample, mnistFeatureDescriptions)\n    return (tf.io.parse_tensor(example[\"image\"], tf.float64), example[\"value\"])\n\ndef load_tfrec_files(filePaths):\n    dataset = tf.data.TFRecordDataset(filePaths)\n    return dataset.map(decode_example).batch(32).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:57:49.787456Z","iopub.execute_input":"2023-03-29T17:57:49.788119Z","iopub.status.idle":"2023-03-29T17:57:49.799992Z","shell.execute_reply.started":"2023-03-29T17:57:49.788055Z","shell.execute_reply":"2023-03-29T17:57:49.798766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = load_tfrec_files(train_paths)\nvalid_dataset = load_tfrec_files(valid_paths)\ntest_dataset = load_tfrec_files(test_paths)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T17:59:07.738643Z","iopub.execute_input":"2023-03-29T17:59:07.739113Z","iopub.status.idle":"2023-03-29T17:59:07.810761Z","shell.execute_reply.started":"2023-03-29T17:59:07.739069Z","shell.execute_reply":"2023-03-29T17:59:07.809465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normalization = keras.layers.Normalization(input_shape = (28, 28))\nadapt_image_batches = train_dataset.take(128).map(lambda image, value: image)\nadapt_images = np.concatenate(list(adapt_image_batches.as_numpy_iterator()), axis = 0)\nnormalization.adapt(adapt_images)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T18:10:44.043002Z","iopub.execute_input":"2023-03-29T18:10:44.043411Z","iopub.status.idle":"2023-03-29T18:10:44.699776Z","shell.execute_reply.started":"2023-03-29T18:10:44.043376Z","shell.execute_reply":"2023-03-29T18:10:44.698735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(normalization)\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(100, activation = \"relu\"))\nmodel.add(keras.layers.Dense(10, activation = \"softmax\"))\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"nadam\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-29T18:11:20.977144Z","iopub.execute_input":"2023-03-29T18:11:20.978220Z","iopub.status.idle":"2023-03-29T18:11:21.036835Z","shell.execute_reply.started":"2023-03-29T18:11:20.978169Z","shell.execute_reply":"2023-03-29T18:11:21.035556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset, epochs = 5, validation_data = valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T18:12:09.947022Z","iopub.execute_input":"2023-03-29T18:12:09.947468Z","iopub.status.idle":"2023-03-29T18:12:53.398351Z","shell.execute_reply.started":"2023-03-29T18:12:09.947423Z","shell.execute_reply":"2023-03-29T18:12:53.396826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exercise 10","metadata":{}},{"cell_type":"code","source":"import os\nimport tarfile\nimport urllib.request\n\ndatadir = os.path.join(\"datasets\", \"largemovie\")\n#os.makedirs(datadir, exist_ok = True)\n\n#req = urllib.request.urlopen(\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n#with tarfile.open(mode = \"r:gz\", fileobj = req) as tar:\n#    tar.extractall(datadir)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:35:42.478577Z","iopub.execute_input":"2023-04-25T16:35:42.479031Z","iopub.status.idle":"2023-04-25T16:35:42.485921Z","shell.execute_reply.started":"2023-04-25T16:35:42.478970Z","shell.execute_reply":"2023-04-25T16:35:42.484329Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import glob\ndef list_paths(dir1, dir2):\n    return glob.glob(os.path.join(datadir, \"aclImdb\", dir1, dir2, \"*.txt\"))\n\ntrain_pos_paths = list_paths(\"train\", \"pos\")\ntrain_neg_paths = list_paths(\"train\", \"neg\")\ntest_all_pos_paths = list_paths(\"test\", \"pos\")\ntest_all_neg_paths = list_paths(\"test\", \"neg\")","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:35:44.943825Z","iopub.execute_input":"2023-04-25T16:35:44.944668Z","iopub.status.idle":"2023-04-25T16:35:45.118266Z","shell.execute_reply.started":"2023-04-25T16:35:44.944620Z","shell.execute_reply":"2023-04-25T16:35:45.116932Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"np.random.shuffle(test_all_pos_paths)\nnp.random.shuffle(test_all_neg_paths)\n\nvalid_pos_paths = test_all_pos_paths[5000:]\nvalid_neg_paths = test_all_neg_paths[5000:]\ntest_pos_paths = test_all_pos_paths[:5000]\ntest_neg_paths = test_all_neg_paths[:5000]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:35:47.666270Z","iopub.execute_input":"2023-04-25T16:35:47.667685Z","iopub.status.idle":"2023-04-25T16:35:47.678732Z","shell.execute_reply.started":"2023-04-25T16:35:47.667625Z","shell.execute_reply":"2023-04-25T16:35:47.677273Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def make_dataset(pos_paths, neg_paths):\n    pos_dataset = tf.data.TextLineDataset(pos_paths).map(lambda x: (x, 1))\n    neg_dataset = tf.data.TextLineDataset(neg_paths).map(lambda x: (x, 0))\n    return tf.data.Dataset.concatenate(pos_dataset, neg_dataset)\n\nbatch_size = 32\ntrain_dataset = make_dataset(train_pos_paths, train_neg_paths).shuffle(25000).batch(batch_size).prefetch(1)\nvalid_dataset = make_dataset(valid_pos_paths, valid_neg_paths).batch(batch_size).prefetch(1)\ntest_dataset = make_dataset(test_pos_paths, test_neg_paths).batch(batch_size).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:01:26.261132Z","iopub.execute_input":"2023-04-25T17:01:26.262266Z","iopub.status.idle":"2023-04-25T17:01:26.493845Z","shell.execute_reply.started":"2023-04-25T17:01:26.262214Z","shell.execute_reply":"2023-04-25T17:01:26.492102Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"list(test_dataset.range(2).as_numpy_iterator())","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:01:24.602121Z","iopub.execute_input":"2023-04-25T17:01:24.602579Z","iopub.status.idle":"2023-04-25T17:01:24.618189Z","shell.execute_reply.started":"2023-04-25T17:01:24.602540Z","shell.execute_reply":"2023-04-25T17:01:24.617122Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"[0, 1]"},"metadata":{}}]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"tf.strings.substr([\"hello te lo\"], 0, 10)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:47:41.637585Z","iopub.execute_input":"2023-04-25T16:47:41.638063Z","iopub.status.idle":"2023-04-25T16:47:41.647385Z","shell.execute_reply.started":"2023-04-25T16:47:41.638021Z","shell.execute_reply":"2023-04-25T16:47:41.646089Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'hello te l'], dtype=object)>"},"metadata":{}}]},{"cell_type":"code","source":"max_tokens = 1024\n\ndef standardize(text):\n    shape = tf.shape(text) # * tf.constant([1, 0]) + tf.constant([0, 50])\n    #print(\"standardize\", text[0], tf.shape(text), shape)    \n    Z = tf.strings.substr(text, 0, 300)\n    #print(\"shape1\", tf.shape(Z))\n    Z = tf.strings.lower(Z)\n    #print(\"shape2\", tf.shape(Z))\n    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n    #print(\"shape3\", tf.shape(Z))\n    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n    #print(\"shape4\", tf.shape(Z))\n    return Z\n    Z = tf.strings.split(Z)\n    print(\"shape5\", tf.shape(Z))\n    #t= Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n    #print(\"shape\", tf_shape(t))\n    return Z\n\ntext_vectorization = tf.keras.layers.TextVectorization(max_tokens = 1024, output_mode = \"count\", pad_to_max_tokens = True,\n                                                       standardize = standardize)\ntrain_batches = train_dataset.map(lambda review, label: review)\ntrain_reviews = np.concatenate(list(train_batches.as_numpy_iterator()), axis = 0)\ntext_vectorization.adapt(train_reviews)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:01:30.467544Z","iopub.execute_input":"2023-04-25T17:01:30.468001Z","iopub.status.idle":"2023-04-25T17:01:36.688605Z","shell.execute_reply.started":"2023-04-25T17:01:30.467961Z","shell.execute_reply":"2023-04-25T17:01:36.687255Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#text_vectorization.get_vocabulary()[:10]\ntext_vectorization.call([\"the the movie was great\", \"and it is not bad\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:52:27.817863Z","iopub.execute_input":"2023-04-25T16:52:27.819091Z","iopub.status.idle":"2023-04-25T16:52:27.834320Z","shell.execute_reply.started":"2023-04-25T16:52:27.818972Z","shell.execute_reply":"2023-04-25T16:52:27.832875Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 1024), dtype=float32, numpy=\narray([[0., 2., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"tf.one_hot([[1, 2, 3, 4], [4, 3, 2, 1]], 5)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:11:35.330662Z","iopub.execute_input":"2023-04-25T15:11:35.331107Z","iopub.status.idle":"2023-04-25T15:11:35.342855Z","shell.execute_reply.started":"2023-04-25T15:11:35.331072Z","shell.execute_reply":"2023-04-25T15:11:35.341395Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 4, 5), dtype=float32, numpy=\narray([[[0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 1.]],\n\n       [[0., 0., 0., 0., 1.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.]]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"tf.reduce_sum(tf.one_hot([[1, 2, 3, 4, 1, 1], [4, 3, 2, 1, 3, 2]], 5), axis = 1)[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:13:52.342713Z","iopub.execute_input":"2023-04-25T15:13:52.343172Z","iopub.status.idle":"2023-04-25T15:13:52.358828Z","shell.execute_reply.started":"2023-04-25T15:13:52.343122Z","shell.execute_reply":"2023-04-25T15:13:52.357509Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\narray([[3., 1., 1., 1.],\n       [1., 2., 2., 1.]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"class WordCount(keras.layers.Layer):\n    def __init__(self, num_tokens, **kwargs):\n        super().__init__(dtype = tf.int64, **kwargs)\n        self._num_tokens = num_tokens\n        \n    def call(self, input):\n        return tf.reduce_sum(tf.one_hot(input, self._num_tokens), axis=1)[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:18:04.516272Z","iopub.execute_input":"2023-04-25T15:18:04.517689Z","iopub.status.idle":"2023-04-25T15:18:04.525115Z","shell.execute_reply.started":"2023-04-25T15:18:04.517629Z","shell.execute_reply":"2023-04-25T15:18:04.523704Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential([\n    text_vectorization,\n    keras.layers.Dense(100, activation = \"relu\"),\n    keras.layers.Dense(1, activation = \"sigmoid\")\n])\nmodel.compile(loss = \"binary_crossentropy\", optimizer = \"nadam\", metrics = [\"accuracy\"])\nmodel.fit(train_dataset, epochs = 5, validation_data = valid_dataset)\n            ","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:01:40.078243Z","iopub.execute_input":"2023-04-25T17:01:40.079019Z","iopub.status.idle":"2023-04-25T17:02:42.091583Z","shell.execute_reply.started":"2023-04-25T17:01:40.078956Z","shell.execute_reply":"2023-04-25T17:02:42.090344Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Epoch 1/5\n782/782 [==============================] - 11s 9ms/step - loss: 0.5341 - accuracy: 0.7263 - val_loss: 0.4931 - val_accuracy: 0.7537\nEpoch 2/5\n782/782 [==============================] - 13s 12ms/step - loss: 0.4721 - accuracy: 0.7698 - val_loss: 0.4873 - val_accuracy: 0.7578\nEpoch 3/5\n782/782 [==============================] - 9s 9ms/step - loss: 0.4348 - accuracy: 0.7932 - val_loss: 0.4912 - val_accuracy: 0.7575\nEpoch 4/5\n782/782 [==============================] - 10s 9ms/step - loss: 0.3832 - accuracy: 0.8252 - val_loss: 0.5053 - val_accuracy: 0.7523\nEpoch 5/5\n782/782 [==============================] - 10s 9ms/step - loss: 0.3204 - accuracy: 0.8628 - val_loss: 0.5232 - val_accuracy: 0.7457\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x75a829fd1150>"},"metadata":{}}]},{"cell_type":"code","source":"tf.math.count_nonzero([[1, 0, 2, 3, 0, 4], [0, 0, 1, 0, 0, 2], [0, 0, 0, 0, 0, 0]], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:46:22.405598Z","iopub.execute_input":"2023-04-25T17:46:22.406023Z","iopub.status.idle":"2023-04-25T17:46:22.416304Z","shell.execute_reply.started":"2023-04-25T17:46:22.405972Z","shell.execute_reply":"2023-04-25T17:46:22.414983Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3,), dtype=int64, numpy=array([4, 2, 0])>"},"metadata":{}}]},{"cell_type":"code","source":"tf.math.count_nonzero([4, 2, 0], axis = 0, keepdims = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:47:49.624128Z","iopub.execute_input":"2023-04-25T17:47:49.624528Z","iopub.status.idle":"2023-04-25T17:47:49.636267Z","shell.execute_reply.started":"2023-04-25T17:47:49.624494Z","shell.execute_reply":"2023-04-25T17:47:49.635083Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>"},"metadata":{}}]},{"cell_type":"code","source":"def compute_embedding_mean(input):\n    num_words = tf.math.count_nonzero(tf.math.count_nonzero(input, axis = -1), axis = -1, keepdims = True)\n    return tf.reduce_sum(input, axis = 1) / tf.math.sqrt(tf.cast(num_words, tf.float32))","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:54:26.035817Z","iopub.execute_input":"2023-04-25T17:54:26.036259Z","iopub.status.idle":"2023-04-25T17:54:26.042691Z","shell.execute_reply.started":"2023-04-25T17:54:26.036221Z","shell.execute_reply":"2023-04-25T17:54:26.041265Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"text_vectorization = tf.keras.layers.TextVectorization(max_tokens = 1024, output_mode = \"int\", standardize = standardize)\ntrain_batches = train_dataset.map(lambda review, label: review)\ntrain_reviews = np.concatenate(list(train_batches.as_numpy_iterator()), axis = 0)\ntext_vectorization.adapt(train_batches)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:57:29.325255Z","iopub.execute_input":"2023-04-25T17:57:29.328813Z","iopub.status.idle":"2023-04-25T17:57:39.649383Z","shell.execute_reply.started":"2023-04-25T17:57:29.328761Z","shell.execute_reply":"2023-04-25T17:57:39.648085Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential([\n    text_vectorization,\n    keras.layers.Embedding(input_dim=max_tokens, output_dim=20, mask_zero=True),\n    keras.layers.Lambda(compute_embedding_mean),\n    keras.layers.Dense(100, activation = \"relu\"),\n    keras.layers.Dense(1, activation = \"sigmoid\")\n\n])\nmodel.compile(loss = \"binary_crossentropy\", optimizer = \"nadam\", metrics = [\"accuracy\"])\nmodel.fit(train_dataset, epochs = 5, validation_data = valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:58:04.635236Z","iopub.execute_input":"2023-04-25T17:58:04.635647Z","iopub.status.idle":"2023-04-25T17:58:55.489072Z","shell.execute_reply.started":"2023-04-25T17:58:04.635613Z","shell.execute_reply":"2023-04-25T17:58:55.488127Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Epoch 1/5\n782/782 [==============================] - 10s 8ms/step - loss: 0.5435 - accuracy: 0.7153 - val_loss: 0.4994 - val_accuracy: 0.7487\nEpoch 2/5\n782/782 [==============================] - 9s 7ms/step - loss: 0.4855 - accuracy: 0.7594 - val_loss: 0.4933 - val_accuracy: 0.7533\nEpoch 3/5\n782/782 [==============================] - 8s 7ms/step - loss: 0.4753 - accuracy: 0.7660 - val_loss: 0.4873 - val_accuracy: 0.7541\nEpoch 4/5\n782/782 [==============================] - 8s 7ms/step - loss: 0.4669 - accuracy: 0.7699 - val_loss: 0.4953 - val_accuracy: 0.7538\nEpoch 5/5\n782/782 [==============================] - 9s 7ms/step - loss: 0.4607 - accuracy: 0.7709 - val_loss: 0.4942 - val_accuracy: 0.7495\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x75a828f9e310>"},"metadata":{}}]},{"cell_type":"code","source":"train_reviews","metadata":{"execution":{"iopub.status.busy":"2023-04-25T18:00:00.495045Z","iopub.execute_input":"2023-04-25T18:00:00.495456Z","iopub.status.idle":"2023-04-25T18:00:00.503304Z","shell.execute_reply.started":"2023-04-25T18:00:00.495420Z","shell.execute_reply":"2023-04-25T18:00:00.501932Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"array([b'This is one of those \"so bad it is good\" films that you always hear about but never see! Unlike Troma films which are deliberately bad and campy (and I am not amused) this one is 100% pure serious.<br /><br />However with features such as a supposedly super-lethal killer robot that prances about like one of the Solid Gold Dancers on an acid trip and a magical first mate that calls down lightning and transforms into the Good Witch of the East the fact that it takes itself seriously pushes it so far over the edge of bad that makes it full circle around back to entertaining.<br /><br />Watcheable enough because of that.',\n       b\"This movie was horrible. I swear they didn't even write a script they just kinda winged it through out the whole movie. Ice-T was annoying as hell. *SPOILERS Phht more like reasons not to watch it* They sit down and eat breakfast for 20 minutes. he coulda been long gone. The ground was hard it would of been close to impossible to to track him with out dogs. And when ICE-T is on that Hill and uses that Spaz-15 Assault SHOTGUN like its a sniper rifle (and then cuts down a tree with eight shells?? It would take 1000's of shells to cut down a tree that size.) Shotguns and hand guns are considered to be inaccurate at 100yards. And they even saw the reflection. What reflected the light?? I didn't see a scope on that thing. Also when he got shot in the gut and kept going, that was retarded he would of bled to death right there. PlusThe ending where he stuffs a rock or a cigarette in the guys barrel. It wouldn't blow up and kill him. The bullet would still fire kill Ice T but mess up the barrel.\",\n       b\"To my surprise I quite enjoyed 'Spacecamp', i remember seeing it about 13 years ago, and recently I hired it again. I was quite impressed. Obviously the special effects in todays space films such as Armageddon and Deep Impact are far superior to those in SpaceCamp. However, this film had a story- a very stereotypical eighties story where you could almost recite the next line of dialogue before hearing it. But thats what I liked about it- they don't make films like this anymore, so it was a refreshing change. It was interesting to see Kelly Preston, Leaf Phoenix and Lea Thomson in early roles, with Tom Skerrit and Kate Capshaw to add substance to the light & fluffy plot. Absolutely loved the robot named Jinx, it was very cute, but it unfortunately had more emotion than some of the main characters. The film was almost inspirational in its own way, and it was interesting to note that it was filmed at the NASA Spacecamp in Alabama (i think).\",\n       ...,\n       b\"With the death of her infirmed husband, May, an older woman faces a future in an urban world that views her as invisible, dead from the neck down, and unwelcome in the pseudo- sophisticated yuppie homes of her son, Bobby and his shallow wife, Helen, and Paula, a self- absorbed, clinging, and minimally talented daughter. The central family is anything but warm, supportive, and understanding of her new and tragic stage in life with the death of her husband. The Mother is a quiet character study that points up how in some societies, the elder parent is both unwelcome and a burden to grown children whose careers and status seeking overshadow all else. <br /><br />As May comes to realize the world is still important to her, the lonely widow finds her libido reawakened and alive with her daughter's boyfriend, a carpenter and rough sort. May embarks on an uninhibited sexual affair with Darren whose character is sympathetic to her at first, but his flawed nature is quickly revealed through the pressures of the women who surround him.<br /><br />This is the kind of role Hollywood actresses of a certain age whine is never written for them, but would never appear in because the film's frankness, overt sexuality, unglamorous wardrobe, little makeup, and social commentary on the vapidness of the very society most film industry women are enchrenched. The performance by the lead actress, Anne Reid ranges from quiet to giddy and her interpretation blossoms on screen from the drab widow to a sexually alive and freed middle age woman without face-lift, hair extensions, and liposuction. She bares more than her soul for the screen.<br /><br />Daniel Craig is the enabling handyman, Derrek who beds both mother and daughter. He turns in another stellar performance that is at first sympathetic to the widow's situation, but in the end is without redemption as his true nature unfold and he is literally the rooster in a hen-house. His aimless character's inability to say no to the ex-wife, boring girlfriend, and her mother is blamed as the root of his ineffectual existence. While good with his hands at building a conservatory, he is unable to construct meaning in his life.<br /><br />One of the best films from Britain in years, it is simply adult in its storyline. The Mother is the rare kind of film that is perhaps too honest for American audiences to tolerate having no car chase, no bling, no rap soundtrack to drown out the cretin performances by TV starlets and buff studmuffins. The Mother reflects how the aging baby boomers are now disposable people that offspring are willing to overlook, send to the retirement home, and get out of the way. May doesn't know what to do as she is made alive by Darren, isn't willing to go to the old folks home, and finds her kids are more conservative than she ever was at their age.\",\n       b\"Lauren Bacall was living through husband Humprey Bogarts illness & death when she did this film. Rock Hudson was near the top of his 1950's stardom. Dorothy Malone is in excellent form, and wins an Oscar for support. Robert Stack is nominated & falls just short for his role.<br /><br />The story is a little soapy from another time but just as worthwhile as most dramas. Amazing how well drunks can drive in this film & also how quickly Stack sobers up in a couple of the films early sequences.<br /><br />You can see why the cast is so good & actually production wise this film is very good. You can tell Bacall is distracted during this film as while her acting is fine, she looks emotionally drained in some sequences.<br /><br />The sexual references in this film are so mild, that many of today's young viewers would not realize what they are. Film does a good job telling a story & actually leaves a sequel to be made at the end though none ever was made- though Written Beyond THe Wind would be a good title.\",\n       b'\\'The Student of Prague\\' is an early feature-length horror drama or, rather, it is an \"autorenfilm\" (i.e. an author\\'s film). This film is a member of a movement of many movements that tried to lend respectability to cin\\xc3\\xa9ma, or just make a profit, by adapting literature or theatre onto the screen. Fortunately, the story of this book with moving pictures is good. Using Alfred de Musset\\'s poem and a story by Edgar Allen Poe, it centres on the doppelg\\xc3\\xa4nger theme.<br /><br />Unfortunately, the most cinematic this film gets is the double exposure effects to make Paul Wegener appear twice within scenes. Guido Seeber was a special effects wizard for his day, but he\\'s not very good at positioning the camera or moving it. Film scholar Leon Hunt (printed in \"Early Cinema: Space, Frame, Narrative\"), however, has made an interesting analysis on this film using framing to amplify the doubles theme: characters being split by left/right, near/far and frontal/diagonal framing of characters and shots. Regardless, the film mostly consists of extended long shots from a fixed position, which is noticeably primitive. Worse is the lack of editing; there\\'s very little scene dissection and scenes linger. None of this is unusual for 1913, but there were more advanced films in this respect around the same time, including the better parts of \\'Atlantis\\' (August Blom, 1913), \\'Twilight of a Woman\\'s Soul\\' (Yevgeni Bauer, 1913) and the short films of D.W. Griffith.<br /><br />An expanded universal film vocabulary by 1926 would allow for a vastly superior remake. Furthermore, the remake has a reason for the Lyduschka character, other than being an occasional troublemaker and spectator surrogate. Here, the obtrusively acted gypsy lurks around, seemingly, with a cloak of invisibility. I know their world is silent to me, but I assume, with their lips moving and such, that their world would not be silent to them, so how can Lyduschka leer over others\\' shoulders and not be noticed?<br /><br />Nevertheless, this is one of the most interesting early films conceptually. Wegener, who seems to have been the primary mind behind this film, in addition to playing the lead, would later play the title role and co-direct \\'The Golem\\' in 1920--helping to further inaugurate the supernatural thread in German silent cin\\xc3\\xa9ma.<br /><br />(Note: The first version I viewed was about an hour long (surely not quite complete) and was in poor condition, with faces bleached at times and such. I\\'m not sure who was the distributor. I\\'ve also since seen the Alpha DVD, which, at 41 minutes, is missing footage present in the aforementioned print and also has fewer and very different title cards, but is visually not as bad. The repetitive score is best muted, though.)'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Exercises\n\n## 1. to 8.\nSee Appendix A\n\n## 9.\n### a.\n_Exercise: Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized `Example` protobuf with two features: the serialized image (use `tf.io.serialize_tensor()` to serialize each image), and the label. Note: for large images, you could use `tf.io.encode_jpeg()` instead. This would save a lot of space, but it would lose a bit of image quality._","metadata":{}},{"cell_type":"code","source":"(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\nvalid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\ntest_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_example(image, label):\n    image_data = tf.io.serialize_tensor(image)\n    #image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n    return Example(\n        features=Features(\n            feature={\n                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n                \"label\": Feature(int64_list=Int64List(value=[label])),\n            }))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image, label in valid_set.take(1):\n    print(create_example(image, label))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following function saves a given dataset to a set of TFRecord files. The examples are written to the files in a round-robin fashion. To do this, we enumerate all the examples using the `dataset.enumerate()` method, and we compute `index % n_shards` to decide which file to write to. We use the standard `contextlib.ExitStack` class to make sure that all writers are properly closed whether or not an I/O error occurs while writing.","metadata":{}},{"cell_type":"code","source":"from contextlib import ExitStack\n\ndef write_tfrecords(name, dataset, n_shards=10):\n    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n             for index in range(n_shards)]\n    with ExitStack() as stack:\n        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n                   for path in paths]\n        for index, (image, label) in dataset.enumerate():\n            shard = index % n_shards\n            example = create_example(image, label)\n            writers[shard].write(example.SerializeToString())\n    return paths","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\nvalid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\ntest_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b.\n_Exercise: Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data._","metadata":{}},{"cell_type":"code","source":"def preprocess(tfrecord):\n    feature_descriptions = {\n        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n    }\n    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n    #image = tf.io.decode_jpeg(example[\"image\"])\n    image = tf.reshape(image, shape=[28, 28])\n    return image, example[\"label\"]\n\ndef mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n                  n_parse_threads=5, batch_size=32, cache=True):\n    dataset = tf.data.TFRecordDataset(filepaths,\n                                      num_parallel_reads=n_read_threads)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.batch(batch_size)\n    return dataset.prefetch(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\nvalid_set = mnist_dataset(valid_filepaths)\ntest_set = mnist_dataset(test_filepaths)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for X, y in train_set.take(1):\n    for i in range(5):\n        plt.subplot(1, 5, i + 1)\n        plt.imshow(X[i].numpy(), cmap=\"binary\")\n        plt.axis(\"off\")\n        plt.title(str(y[i].numpy()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nclass Standardization(keras.layers.Layer):\n    def adapt(self, data_sample):\n        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n    def call(self, inputs):\n        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n\nstandardization = Standardization(input_shape=[28, 28])\n# or perhaps soon:\n#standardization = keras.layers.Normalization()\n\nsample_image_batches = train_set.take(100).map(lambda image, label: image)\nsample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n                               axis=0).astype(np.float32)\nstandardization.adapt(sample_images)\n\nmodel = keras.models.Sequential([\n    standardization,\n    keras.layers.Flatten(),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"nadam\", metrics=[\"accuracy\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nlogs = os.path.join(os.curdir, \"my_logs\",\n                    \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n\ntensorboard_cb = tf.keras.callbacks.TensorBoard(\n    log_dir=logs, histogram_freq=1, profile_batch=10)\n\nmodel.fit(train_set, epochs=5, validation_data=valid_set,\n          callbacks=[tensorboard_cb])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Warning:** The profiling tab in TensorBoard works if you use TensorFlow 2.2+. You also need to make sure `tensorboard_plugin_profile` is installed (and restart Jupyter if necessary).","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir=./my_logs --port=6006","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.\n_Exercise: In this exercise you will download a dataset, split it, create a `tf.data.Dataset` to load it and preprocess it efficiently, then build and train a binary classification model containing an `Embedding` layer._\n\n### a.\n_Exercise: Download the [Large Movie Review Dataset](https://homl.info/imdb), which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/). The data is organized in two directories, `train` and `test`, each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise._","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nDOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\nFILENAME = \"aclImdb_v1.tar.gz\"\nfilepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\npath = Path(filepath).parent / \"aclImdb\"\npath","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, subdirs, files in os.walk(path):\n    indent = len(Path(name).parts) - len(path.parts)\n    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n    for index, filename in enumerate(sorted(files)):\n        if index == 3:\n            print(\"    \" * (indent + 1) + \"...\")\n            break\n        print(\"    \" * (indent + 1) + filename)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def review_paths(dirpath):\n    return [str(path) for path in dirpath.glob(\"*.txt\")]\n\ntrain_pos = review_paths(path / \"train\" / \"pos\")\ntrain_neg = review_paths(path / \"train\" / \"neg\")\ntest_valid_pos = review_paths(path / \"test\" / \"pos\")\ntest_valid_neg = review_paths(path / \"test\" / \"neg\")\n\nlen(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b.\n_Exercise: Split the test set into a validation set (15,000) and a test set (10,000)._","metadata":{}},{"cell_type":"code","source":"np.random.shuffle(test_valid_pos)\n\ntest_pos = test_valid_pos[:5000]\ntest_neg = test_valid_neg[:5000]\nvalid_pos = test_valid_pos[5000:]\nvalid_neg = test_valid_neg[5000:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### c.\n_Exercise: Use tf.data to create an efficient dataset for each set._","metadata":{}},{"cell_type":"markdown","source":"Since the dataset fits in memory, we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`:","metadata":{}},{"cell_type":"code","source":"def imdb_dataset(filepaths_positive, filepaths_negative):\n    reviews = []\n    labels = []\n    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n        for filepath in filepaths:\n            with open(filepath) as review_file:\n                reviews.append(review_file.read())\n            labels.append(label)\n    return tf.data.Dataset.from_tensor_slices(\n        (tf.constant(reviews), tf.constant(labels)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for X, y in imdb_dataset(train_pos, train_neg).take(3):\n    print(X)\n    print(y)\n    print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It takes about 17 seconds to load the dataset and go through it 10 times.","metadata":{}},{"cell_type":"markdown","source":"But let's pretend the dataset does not fit in memory, just to make things more interesting. Luckily, each review fits on just one line (they use `<br />` to indicate line breaks), so we can read the reviews using a `TextLineDataset`. If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords). For very large datasets, it would make sense to use a tool like Apache Beam for that.","metadata":{}},{"cell_type":"code","source":"def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n                                          num_parallel_reads=n_read_threads)\n    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n                                          num_parallel_reads=n_read_threads)\n    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it takes about 33 seconds to go through the dataset 10 times. That's much slower, essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch. If you add `.cache()` just before `.repeat(10)`, you will see that this implementation will be about as fast as the previous one.","metadata":{}},{"cell_type":"code","source":"%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ntrain_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\nvalid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\ntest_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### d.\n_Exercise: Create a binary classification model, using a `TextVectorization` layer to preprocess each review. If the `TextVectorization` layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package, for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces, and `split()` to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._","metadata":{}},{"cell_type":"markdown","source":"Let's first write a function to preprocess the reviews, cropping them to 300 characters, converting them to lower case, then replacing `<br />` and all non-letter characters to spaces, splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly `n_words` tokens:","metadata":{}},{"cell_type":"code","source":"def preprocess(X_batch, n_words=50):\n    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n    Z = tf.strings.substr(X_batch, 0, 300)\n    Z = tf.strings.lower(Z)\n    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n    Z = tf.strings.split(Z)\n    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n\nX_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\npreprocess(X_example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's write a second utility function that will take a data sample with the same format as the output of the `preprocess()` function, and will output the list of the top `max_size` most frequent words, ensuring that the padding token is first:","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef get_vocabulary(data_sample, max_size=1000):\n    preprocessed_reviews = preprocess(data_sample).numpy()\n    counter = Counter()\n    for words in preprocessed_reviews:\n        for word in words:\n            if word != b\"<pad>\":\n                counter[word] += 1\n    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n\nget_vocabulary(X_example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to create the `TextVectorization` layer. Its constructor just saves the hyperparameters (`max_vocabulary_size` and `n_oov_buckets`). The `adapt()` method computes the vocabulary using the `get_vocabulary()` function, then it builds a `StaticVocabularyTable` (see Chapter 16 for more details). The `call()` method preprocesses the reviews to get a padded list of words for each review, then it uses the `StaticVocabularyTable` to lookup the index of each word in the vocabulary:","metadata":{}},{"cell_type":"code","source":"class TextVectorization(keras.layers.Layer):\n    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self.max_vocabulary_size = max_vocabulary_size\n        self.n_oov_buckets = n_oov_buckets\n\n    def adapt(self, data_sample):\n        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n        words = tf.constant(self.vocab)\n        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n        \n    def call(self, inputs):\n        preprocessed_inputs = preprocess(inputs)\n        return self.table.lookup(preprocessed_inputs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try it on our small `X_example` we defined earlier:","metadata":{}},{"cell_type":"code","source":"text_vectorization = TextVectorization()\n\ntext_vectorization.adapt(X_example)\ntext_vectorization(X_example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks good! As you can see, each review was cleaned up and tokenized, then each word was encoded as its index in the vocabulary (all the 0s correspond to the `<pad>` tokens).\n\nNow let's create another `TextVectorization` layer and let's adapt it to the full IMDB training set (if the training set did not fit in RAM, we could just use a smaller sample of the training set by calling `train_set.take(500)`):","metadata":{}},{"cell_type":"code","source":"max_vocabulary_size = 1000\nn_oov_buckets = 100\n\nsample_review_batches = train_set.map(lambda review, label: review)\nsample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n                                axis=0)\n\ntext_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n                                       input_shape=[])\ntext_vectorization.adapt(sample_reviews)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's run it on the same `X_example`, just to make sure the word IDs are larger now, since the vocabulary is bigger:","metadata":{}},{"cell_type":"code","source":"text_vectorization(X_example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good! Now let's take a look at the first 10 words in the vocabulary:","metadata":{}},{"cell_type":"code","source":"text_vectorization.vocab[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are the most common words in the reviews.","metadata":{}},{"cell_type":"markdown","source":"Now to build our model we will need to encode all these word IDs somehow. One approach is to create bags of words: for each review, and for each word in the vocabulary, we count the number of occurences of that word in the review. For example:","metadata":{}},{"cell_type":"code","source":"simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\ntf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first review has 2 times the word 0, 2 times the word 1, 0 times the word 2, and 1 time the word 3, so its bag-of-words representation is `[2, 2, 0, 1]`. Similarly, the second review has 3 times the word 0, 0 times the word 1, and so on. Let's wrap this logic in a small custom layer, and let's test it. We'll drop the counts for the word 0, since this corresponds to the `<pad>` token, which we don't care about.","metadata":{}},{"cell_type":"code","source":"class BagOfWords(keras.layers.Layer):\n    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        self.n_tokens = n_tokens\n    def call(self, inputs):\n        one_hot = tf.one_hot(inputs, self.n_tokens)\n        return tf.reduce_sum(one_hot, axis=1)[:, 1:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's test it:","metadata":{}},{"cell_type":"code","source":"bag_of_words = BagOfWords(n_tokens=4)\nbag_of_words(simple_example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It works fine! Now let's create another `BagOfWord` with the right vocabulary size for our training set:","metadata":{}},{"cell_type":"code","source":"n_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for <pad>\nbag_of_words = BagOfWords(n_tokens)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're ready to train the model!","metadata":{}},{"cell_type":"code","source":"model = keras.models.Sequential([\n    text_vectorization,\n    bag_of_words,\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=5, validation_data=valid_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get about 73.5% accuracy on the validation set after just the first epoch, but after that the model makes no significant progress. We will do better in Chapter 16. For now the point is just to perform efficient preprocessing using `tf.data` and Keras preprocessing layers.","metadata":{}},{"cell_type":"markdown","source":"### e.\n_Exercise: Add an `Embedding` layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model._","metadata":{}},{"cell_type":"markdown","source":"To compute the mean embedding for each review, and multiply it by the square root of the number of words in that review, we will need a little function. For each sentence, this function needs to compute $M \\times \\sqrt N$, where $M$ is the mean of all the word embeddings in the sentence (excluding padding tokens), and $N$ is the number of words in the sentence (also excluding padding tokens). We can rewrite $M$ as $\\dfrac{S}{N}$, where $S$ is the sum of all word embeddings (it does not matter whether or not we include the padding tokens in this sum, since their representation is a zero vector). So the function must return $M \\times \\sqrt N = \\dfrac{S}{N} \\times \\sqrt N = \\dfrac{S}{\\sqrt N \\times \\sqrt N} \\times \\sqrt N= \\dfrac{S}{\\sqrt N}$.","metadata":{}},{"cell_type":"code","source":"def compute_mean_embedding(inputs):\n    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n\nanother_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\ncompute_mean_embedding(another_example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check that this is correct. The first review contains 2 words (the last token is a zero vector, which represents the `<pad>` token). Let's compute the mean embedding for these 2 words, and multiply the result by the square root of 2:","metadata":{}},{"cell_type":"code","source":"tf.reduce_mean(another_example[0:1, :2], axis=1) * tf.sqrt(2.)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks good! Now let's check the second review, which contains just one word (we ignore the two padding tokens):","metadata":{}},{"cell_type":"code","source":"tf.reduce_mean(another_example[1:2, :1], axis=1) * tf.sqrt(1.)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect. Now we're ready to train our final model. It's the same as before, except we replaced the `BagOfWords` layer with an `Embedding` layer followed by a `Lambda` layer that calls the `compute_mean_embedding` layer:","metadata":{}},{"cell_type":"code","source":"embedding_size = 20\n\nmodel = keras.models.Sequential([\n    text_vectorization,\n    keras.layers.Embedding(input_dim=n_tokens,\n                           output_dim=embedding_size,\n                           mask_zero=True), # <pad> tokens => zero vectors\n    keras.layers.Lambda(compute_mean_embedding),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### f.\n_Exercise: Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible._","metadata":{}},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nmodel.fit(train_set, epochs=5, validation_data=valid_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is not better using embeddings (but we will do better in Chapter 16). The pipeline looks fast enough (we optimized it earlier).","metadata":{}},{"cell_type":"markdown","source":"### g.\n_Exercise: Use TFDS to load the same dataset more easily: `tfds.load(\"imdb_reviews\")`._","metadata":{}},{"cell_type":"code","source":"import tensorflow_datasets as tfds\n\ndatasets = tfds.load(name=\"imdb_reviews\")\ntrain_set, test_set = datasets[\"train\"], datasets[\"test\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example in train_set.take(1):\n    print(example[\"text\"])\n    print(example[\"label\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}